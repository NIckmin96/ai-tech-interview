<div align='center'>
  <h1>🤖 Machine Learning 🤖</h1>
</div>


## Questions ❓

- [알고 있는 metric에 대해 설명해주세요. (ex. RMSE, MAE, recall, precision ...)](#no1)
- [정규화를 왜 해야할까요? 정규화의 방법은 무엇이 있나요?](#no2)
- [Local Minima와 Global Minima에 대해 설명해주세요.](#no3)
- [차원의 저주에 대해 설명해주세요.](#no4)
- [dimension reduction기법으로 보통 어떤 것들이 있나요?](#no5)
- [PCA는 차원 축소 기법이면서, 데이터 압축 기법이기도 하고, 노이즈 제거기법이기도 합니다. 왜 그런지 설명해주실 수 있나요?](#no6)
- [LSA, LDA, SVD 등의 약자들이 어떤 뜻이고 서로 어떤 관계를 가지는지 설명할 수 있나요?](#no7)
- [Markov Chain을 고등학생에게 설명하려면 어떤 방식이 제일 좋을까요?](#no8)
- [텍스트 더미에서 주제를 추출해야 합니다. 어떤 방식으로 접근해 나가시겠나요?](#no9)


---

### No.1

**알고 있는 metric에 대해 설명해주세요. (ex. RMSE, MAE, recall, precision ...)**

1. RMSE-like metrics
   * [MAE](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanAbsoluteError): Mean Absolute Error, 오차 평균; 오차의 단위가 실제 타겟값의 단위와 똑같다는 특징이 있으며 이상치에 덜 민감하다.
   * [RMSE](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/RootMeanSquaredError): Root Mean Squared Error, 오차 제곱 평균의 제곱근; MAE와 마찬가지로 오차의 단위가 실제 타겟값의 단위와 똑같아서 직관적이다. MAE와는 달리 큰 오차값이 더 반영되어 이상치에 더 민감하다는 특징을 갖고 있다.
   * [MSE](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanSquaredError): Mean Squared Error, 오차 제곱 평균; RMSE와 같이 큰 오차값을 더 반영하는 평가지표지만, 실제 타겟값과 단위가 달라서 직관적이지는 않다. MAE, RMSE와 같이 회귀 문제에서 자주 사용되는 평가지표다.
   * [MAPE](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanAbsolutePercentageError): Mean Absolute Percentage Error, 오차 평균 백분율; 오차를 실제값 y로 나누어 MAE를 백분율 단위로 변경한 것으로, MAE와 많은 공통점을 지니지만 y가 0일 때 사용할 수 없다는 단점이 추가로 존재한다. 백분율 단위의 직관성이 장점이지만, 예측값과 실제값의 대소관계가 지표의 크기에 반영이 되는 특징이 있어 주의가 필요하다. (예/ n=1일 때, 예측값:10/실제값:20 → MAPE = 50% ; 예측값:20/실제값:10 → MAPE = 100%)
2. confusion matrix related metrics
   * [Precision](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision)(정밀도): True Positives / Predicted Positives = True Positives / (True Positives + False Positives): 1종 오류와 관련이 있는 분류 평가 지표. (예/ 스팸 방지 - 스팸메일이 받은메일함으로 분류되는 것보다 정상 메일이 스팸메일함으로 분류되는 것을 더 지양)
   * [Recall](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall) = Sensitivity(민감도): True Positives / Real Positives = True Positives / (True Positives + False Negatives); 2종 오류와 관련이 있는 분류 평가 지표. (예/ 의료 데이터 분석 - 양성으로 잘못 분류하는 것보다 음성으로 잘못 분류하여 환자를 놓치는 경우를 더 지양)
   * Specificity(특이도): True Negatives / Real Negatives = True Negatives / (True Negatives + False Positives); False Alarm(False Positive)를 방지하고자 할때 사용. (예/ 마약복용자를 구속하지 않는 것보다 일반인을 잘못 구속하는 경우를 더 지양)
   * [Precision at Recall](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/PrecisionAtRecall) / [Recall at Precision](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/RecallAtPrecision) : 특정 Recall 값에서의 Precision 측정 또는 특정 Precision 값에서의 Recall 측정
3. [AUC](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/AUC)
   * ROC-AUC: ROC(Receiver Operating Characteristic) 곡선 아래의 면적(0~1); True Positive Rate(=Recall=TP/(TP+FN))과 False Positive Rate(=FP/(FP+TN))의 tradeoff 관계를 시각화하는 지표; 랜덤한 이진 분류의 경우 ROC-AUC 값은 0.5.
   * PR-AUC: Precision-Recall 곡선 아래의 면적(0~1); 음악 장르와 같이 대부분이 0으로 이루어진 (imbalanced 한) 레이블 벡터 y를 타겟으로 할 때는 PR-AUC가 ROC-AUC보다 더 정확한 직관을 주는 것으로 알려져 있음. (True Negative의 영향 때문)
4. crossentropy
   * [binary crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/BinaryCrossentropy): 이진분류 문제에 사용할 수 있는 crossentropy 기반 분류 지표.
   * [categorical crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/CategoricalCrossentropy): multi-label분류 문제에 사용할 수 있는 crossentropy 기반 분류 지표.

참고문헌
- [TensorFlow tf.keras.metrics API docs](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)
- [Regression Metrics for Machine Learning](https://machinelearningmastery.com/regression-metrics-for-machine-learning/)
- [Tutorial: Understanding Regression Error Metrics in Python](https://www.dataquest.io/blog/understanding-regression-error-metrics/)
- [Accuracy, Recall, Precision, F-Score & Specificity, which to optimize on?](https://towardsdatascience.com/accuracy-recall-precision-f-score-specificity-which-to-optimize-on-867d3f11124)
- [F1 Score vs ROC AUC vs Accuracy vs PR AUC: Which Evaluation Metric Should You Choose?](https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc)
- [이진 분류기 성능 평가방법 AUC(area under the ROC curve)의 이해.txt](https://bskyvision.com/1165)
- [Cross-entropy for classification](https://towardsdatascience.com/cross-entropy-for-classification-d98e7f974451)

---
### No.2
**정규화를 왜 해야할까요? 정규화의 방법은 무엇이 있나요?**

다양한 정규화 방법들을 통해서 기계학습 알고리즘의 성능을 증가시킬 수 있으며, 알고리즘들마다 정규화의 효과가 다를 수 있어 실험을 해보는 것이 좋다. 거리기반의 SVM, KNN 알고리즘과 MLP 알고리즘은 특히 정규화의 효능이 좋은 편이다.

1. Normalization (min-max normalization): 값들을 0에서 1사이 범위로 rescaling하는 것을 뜻한다.
2. Standardization (Z-score standardization): 평균 0 표준편차 1의 분포를 띄도록 rescaling하는 것을 뜻한다.

특히 딥러닝에서 정규화는 오랜 기간동안 연구 대상으로 주목 받고 있었으며, 정규화를 수행하는 방향 또한 중요하다고 알려져 있다.

![normalization](https://miro.medium.com/max/1400/1*r0HM4TvZvvceXcJIpDJmDQ.png)

1. Batch Normalization: mini-batch 내 같은 feature들끼리 정규화를 수행하는 방법이다. 보통 CNN 모델에서 많이 사용된다. batch 사이자가 너무 작을 때는 오히려 악역향을 끼칠 수 있다고도 한다.
2. Layer Normalization: 각 데이터 포인트 전체에 대하여 정규화를 수행하는 방법이다. 보통 RNN 또는 Transformer 모델에서 많이 사용된다.

  참고문헌
  - [Normalization vs Standardization — Quantitative analysis](https://towardsdatascience.com/normalization-vs-standardization-quantitative-analysis-a91e8a79cebf)
  - [Normalization Techniques in Deep Neural Networks](https://medium.com/techspace-usict/normalization-techniques-in-deep-neural-networks-9121bf100d8)

---

### No.3
**Local Minima와 Global Minima에 대해 설명해주세요.**

(손실) 함수 전체의 가장 최소값이 global minima이며, gradient decent 등의 최적화 알고리즘으로 찾을 수 있는 global minima 외 최소값들을 local minima라고 한다. 기계학습 최적화 알고리즘들은 local minima에 빠져서 global minima를 찾지 못할 가능성이 항상 존재한다.

![animation](https://vitalflux.com/wp-content/uploads/2020/10/local_minima_vs_global_minima.gif)

global minima를 올바르게 찾기 위해서는 feature engineering을 조심히 히야하며, 다양한 learning rate schedule과 여러 step수를 실험해볼 필요가 있다.

참고문헌

- [Local & Global Minima Explained with Examples](https://vitalflux.com/local-global-maxima-minima-explained-examples/)
---

### No.4
**차원의 저주에 대해 설명해주세요.**

`차원의 저주`란 데이터 학습을 위해 차원이 증가(=변수의 수 증가)하면서 학습 데이터의 수가 차원의 수보다 적어져 성능이 저하되는 현상을 말한다. 즉, 간단히 말하면 차원이 증가함에 따라 모델의 성능이 안 좋아지는 현상을 의미한다.

무조건 변수의 수가 증가한다고 해서 차원의 저주 문제가 있는 것이 아니라, `관측치 수보다 변수의 수가 많아지면` 발생한다. 예를 들어, 관측치의 수는 200개인데, 변수는 7000개인 경우 차원의 저주가 발생하게 된다.

<img src="https://user-images.githubusercontent.com/57162812/150798251-f38d8347-b146-44ee-9056-15194951a7d3.png" width=600>

차원이 늘어날 때마다 점들 사이의 간격이 벌어지게 된다. 즉, 빈 공간이 생기게 되는데 이는 컴퓨터 상에서 0으로 채워졌다는 뜻으로 정보가 없는 셈이다. 정보가 적기 때문에 모델을 돌릴 때 성능이 저하되게 된다. 여기서 빈 공간이 생기는 것을 차원의 저주라고 한다.

차원의 저주를 해결할 수 있는 방법에는 차원을 축소시키거나 데이터를 많이 획득하는 것이 있다.

참고문헌
- [[빅데이터] 차원의 저주(The curse of dimensionality)](https://datapedia.tistory.com/15)

---

### No.5
**dimension reduction기법으로 보통 어떤 것들이 있나요?**

`차원 축소` : 매우 많은 피처로 구성된 다차원 데이터 세트의 타원을 축소해 새로운 차원의 데이터 세트를 생성하는 것

차원 축소는 **피처 선택**과 **피처 추출**로 나눌 수 있다.

`피처 선택`은 특정 피처에 종속성이 강한 불필요한 피처는 아예 제거하는 것이며, `피처 추출`은 기존 피처를 저차원의 중요 피처로 압축해서 추출하는 것이다.

피쳐 선택의 장점은 선택한 피처의 해석이 용이하다는 점이고, 단점은 피처간의 상관관계를 고려하기 어렵다는 점이다.

피처 추출의 장점은 피처 간 상관관계를 고려하기 용이하며 피처의 개수를 많이 줄일 수 있다는 점이고, 단점은 추출된 변수의 해석이 어렵다는 점이다. 대표적인 피처 추출 알고리즘으로는 PCA, SVD, NMF, LDA 등이 있다.

참고 문헌
- [차원 축소 (Dimension Reduction) - PCA, LDA](https://casa-de-feel.tistory.com/19)

---
### No.6
**PCA는 차원 축소 기법이면서, 데이터 압축 기법이기도 하고, 노이즈 제거기법이기도 합니다. 왜 그런지 설명해주실 수 있나요?**

주성분 분석은 차원이 큰 벡터에서 선형 독립하는 고유 벡터만을 남겨두고 차원을 축소하게 된다. 이때 상관성이 높은 독립 변수들을 N개의 선형 조합으로 만들며 개수를 요약, 압축해내는 기법이다. 그리고 이 압축된 각각의 돌깁 변수들은 선형 독립, 즉 직교하며 낮은 상관성을 보이게 된다. 또한 정보 설명력이 높은 주성분들만 선택하면서 정보 설명력이 낮은, 노이즈로 구성된 칼럼들을 배제하기 때문에 노이즈 제거 기법이라고도 불린다.

참고 문헌
- [[기술면접] 차원축소, PCA, SVD, LSA, LDA, MF 간단정리 (day1 / 201009) - Hui_dea](https://huidea.tistory.com/126)

---

### No.7
**LSA, LDA, SVD 등의 약자들이 어떤 뜻이고 서로 어떤 관계를 가지는지 설명할 수 있나요?**

LSA와 LDA 자연어 처리 분야에서 토픽 모델링과 관련이 있고, 이 두 개의 모델링 중 LSA에 SVD가 사용된다.(토픽 모델링이란 텍스트 본문의 숨겨진 의미 구조를 발견하기 위해 사용되는 텍스트 마이닝 기법이다.) 따라서 LSA와 LDA를 설명함에 앞서 SVD를 먼저 이해할 필요가 있다. 

**SVD(Singular Value Decomposition)**: 특이값 분해(SVD)는 행렬을 특정한 구조로 분해하는 방식이다. 3개의 구조로 분해되는데, 이 중 기존 데이터의 정보량을 의미하는 행렬의 원소들을 특이값(singular value)이라고 불러 SVD라고 부른다.  

<img src="https://latex.codecogs.com/svg.image?m&space;\times&space;n" title="m \times n" /> 크기의 행렬 <img src="https://latex.codecogs.com/svg.image?A" title="A" />가 있다고 하자. 행렬 <img src="https://latex.codecogs.com/svg.image?A" title="A" />는 SVD를 통해 아래와 같이 세 개의 행렬로 나누어진다.

<img src="https://latex.codecogs.com/svg.image?A_{[m&space;\times&space;n]}=U_{[m&space;\times&space;r]}&space;\times&space;\sum_{[r&space;\times&space;r]}&space;\times&space;(V_{[n&space;\times&space;r]})^T" title="A_{[m \times n]}=U_{[m \times r]} \times \sum_{[r \times r]} \times (V_{[n \times r]})^T" />  

여기서 각 행렬은 다음과 같은 성질을 가진다.

* <img src="https://latex.codecogs.com/svg.image?A" title="A" /> : <img src="https://latex.codecogs.com/svg.image?m&space;\times&space;n" title="m \times n" /> 크기의 입력 행렬(Input data)  
   * <img src="https://latex.codecogs.com/svg.image?m" title="m" />개의 문서, <img src="https://latex.codecogs.com/svg.image?n" title="n" />개의 단어. 행렬의 값들은 각 문서에 해당 단어가 등장하는 빈도수  

</br>

* <img src="https://latex.codecogs.com/svg.image?U" title="U" /> : <img src="https://latex.codecogs.com/svg.image?m&space;\times&space;r" title="m \times r" /> 크기의 직교 행렬<img src="https://latex.codecogs.com/svg.image?(U&space;\times&space;U^T&space;=&space;I)" title="(U \times U^T = I)" />.
   * <img src="https://latex.codecogs.com/svg.image?r" title="r" /> 컨셉의 중요도

</br>

* <img src="https://latex.codecogs.com/svg.image?&space;\sum&space;" title=" \sum " /> : <img src="https://latex.codecogs.com/svg.image?r&space;\times&space;r" title="r \times r" /> 크기의 대각 행렬. 좌상부터 descending order로 값들이 저장된다(중요도 순).
   * <img src="https://latex.codecogs.com/svg.image?m" title="m" />개의 문서, <img src="https://latex.codecogs.com/svg.image?r" title="r" /> 개의 컨셉  

</br>

* <img src="https://latex.codecogs.com/svg.image?V" title="V" /> : <img src="https://latex.codecogs.com/svg.image?n\times&space;r" title="n\times r" /> 크기의 직교 행렬<img src="https://latex.codecogs.com/svg.image?(V&space;\times&space;V^T&space;=&space;I)" title="(V \times V^T = I)" />.
   * <img src="https://latex.codecogs.com/svg.image?n" title="n" />개의 단어, <img src="https://latex.codecogs.com/svg.image?r" title="r" /> 개의 컨셉 

앞서 SVD는 행렬의 정보를 압축시킬 수 있다고 했는데, 아래 이미지처럼 <img src="https://latex.codecogs.com/svg.image?r" title="r" /> 개의 컨셉 중 상위 <img src="https://latex.codecogs.com/svg.image?k" title="k" />개의 중요한 컨셉을 추출함으로써 이루어진다. <img src="https://latex.codecogs.com/svg.image?\sum_{[r&space;\times&space;r]}" title="\sum_{[r \times r]}" />의 행렬을 <img src="https://latex.codecogs.com/svg.image?\sum_{[k&space;\times&space;k]}" title="\sum_{[k \times k]}" /> 행렬로 줄여도 기존의 <img src="https://latex.codecogs.com/svg.image?m&space;\times&space;n" title="m \times n" /> 크기를 유지할 수 있기 때문이다. 중요도가 낮은 컨셉(노이즈)을 제거함으로써 데이터를 압축할 수 있는 것이다. 이처럼 덜 중요하다고 생각하는 정보를 제거하는 방식을 Truncated SVD라고 부른다.

<img width="500" alt="svd-2" src="https://user-images.githubusercontent.com/63924704/150988716-98bc4ed2-9234-412c-9cca-881acd30f924.png">

</br>

SVD는 이미지 압축에서도 사용된다. 이미지를 정보량(<img src="https://latex.codecogs.com/svg.image?\sum" title="\sum" />의 대각 원소. singular value 혹은 scaling factor라고 부름)에 따라 중요한 정보를 위주로 남기고, 나머지를 버리기 때문에 이미지를 압축하면 (d)와 같이 화질이 깨져 보이게 된다.

<img width="300" alt="svd-4" src="https://user-images.githubusercontent.com/63924704/150988771-0017af6d-6a6c-4761-9623-06ae8464569d.png">


**LSA(Latent Semantic Analysis, 잠재 의미 분석)**: 빈도주의에 기반한 Bag-of-Words(BOW)를 사용하여 문서-단어 행렬(Document-Term Matrix, DTM)로 구성된 데이터의 차원 축소기법으로써, 데이터의 숨어있는 의미를 효과적으로 추출하는 방법이다. 여기서 문서 단어 행렬이란 다수의 문서에서 등장하는 각 단어들의 빈도를 행렬로 표현한 것을 말한다. 쉽게 말해 각 문서에 등장하는 단어의 빈도수를 행렬로 표현한 것이다. LSA는 앞서 소개한 Truncated SVD의 방법을 그대로 이용한다. 

1. SVD를 통해 <img src="https://latex.codecogs.com/svg.image?m&space;\times&space;n" title="m \times n" /> 크기의 단어 정보들을 담은 행렬을 <img src="https://latex.codecogs.com/svg.image?U,&space;\sum,&space;V^T" title="U, \sum, V^T" />로 분해
2. 중요한 정보만 남기기
3. 축소된 차원의 행렬을 사용하여 정보가 압축된 새로운 문서로 데이터에 숨어있는 의미를 효과적으로 추출(의미 정보 함축)

[svd-3]

**LDA(Latent Dirichlet Allocation, 잠재 디리클레 할당)**: 단어가 특정 토픽에 존재할 확률과 문서에 특정 토픽이 존재할 확률을 결합확률로 추정하여 토픽을 추출하는 토픽 모델링 알고리즘 중 하나이다.

정리하면, LSA와 LDA는 모두 문서 내 단어의 빈도수를 활용하여 분석하는데, LSA는 행렬 분해 기반의 정보 압축을 통해, LDA는 확률 기반의 모델링을 통해 이루어진다. 두 방법 모두 단어의 순서는 고려하지 않는다.

**[추가]**
* **LDA(Linear Discriminant Analysis)** : 차원축소기법 중 하나로 분류하기 쉽도록 클래스 간 분산을 최대화하고 클래스 내부의 분산은 최소화하는 방식을 말한다.

**[유용한 자료]**
- [특이값 분해(SVD) - 공돌이의 수학정리노트](https://angeloyeo.github.io/2019/08/01/SVD.html)
- [StatQuest: Linear Discriminant Analysis (LDA) clearly explained.](https://www.youtube.com/watch?v=azXCzI57Yfc&t=779s)

</br>

참고 문헌
- [위키백과 - 특이값 분해](https://ko.wikipedia.org/wiki/%ED%8A%B9%EC%9E%87%EA%B0%92_%EB%B6%84%ED%95%B4)
- [데이터과학 유망주의 매일 글쓰기](https://conanmoon.medium.com/%EB%8D%B0%EC%9D%B4%ED%84%B0%EA%B3%BC%ED%95%99-%EC%9C%A0%EB%A7%9D%EC%A3%BC%EC%9D%98-%EB%A7%A4%EC%9D%BC-%EA%B8%80%EC%93%B0%EA%B8%B0-59-f12da22613fd)
- [[토픽 모델링] LSA와 LDA의 관계 (+ LDA라는 이름의 유래)](https://bab2min.tistory.com/585)
- [문서 단어 행렬(Document-Term Matrix, DTM)](https://settlelib.tistory.com/61)

---

### No.8
**Markov Chain을 고등학생에게 설명하려면 어떤 방식이 제일 좋을까요?**

**마르코프 체인**이란 **마르코프 성질**을 지닌 **이산 확률 과정**을 의미한다. 

* 먼저 **확률 과정**은 시간에 따라 어떤 사건이 발생할 확률이 변화하는 과정을 의미한다.  
* **이산** 시간은 시간이 연속적으로 변하지 않고 이산적으로 변함을 의미한다.  
* **마코프 특성**은 과거 상태들(<img src="https://latex.codecogs.com/svg.image?s_1,&space;s_2,&space;...&space;,&space;s_{t-1}" title="s_1, s_2, ... , s_{t-1}" />)과 현재 상태(<img src="https://latex.codecogs.com/svg.image?s_t" title="s_t" />)가 주어졌을 때, 미래 상태(<img src="https://latex.codecogs.com/svg.image?s_{t&plus;1}" title="s_{t+1}" />)는 과거 상태와는 독립적으로 현재 상태에 의해서만 결정된다는 것을 의미한다.  

즉, 과거와 현재 상태 모두를 고려했을 때 미래 상태가 나타날 확률과 현재 상태만을 고려했을 때 미래 상태가 발생할 확률이 동일하다는 것이다. 이를 식으로 나타내면 다음과 같다.

<img src="https://latex.codecogs.com/svg.image?P(s_{t&plus;1}|s_t)&space;=&space;P(s_{t&plus;1}|s_t,&space;s_{t-1},&space;...&space;s_t)" title="P(s_{t+1}|s_t) = P(s_{t+1}|s_t, s_{t-1}, ... s_t)" />

마코프 체인은 이처럼 과거 상태를 기억하지 않기 때문에 메모리리스(memoryless) 프로세스로 불린다.

참고 문헌
- [MLWiki - Markov Chain](https://sites.google.com/site/machlearnwiki/RBM/markov-chain)
- [[강화학습] 마코프 프로세스(=마코프 체인) 제대로 이해하기](https://bskyvision.com/573)

---

### No.9
**텍스트 더미에서 주제를 추출해야 합니다. 어떤 방식으로 접근해 나가시겠나요?**

LDA(Latent Dirichlet Allocation) 알고리즘으로 접근할 수 있다. LDA는 단어 이면에 존재하는 정보를 추론하여 해당 단어가 어떤 주제에서 뽑힌 것인지 알아내는 과정이다. 

LDA는 문서 내의 토픽 분포(확률)와 각 토픽 내의 단어 분포(확률)을 추정한다(추정 시 사용하는 분포가 Dirichlet 분포이기 때문에 LDA라고 불림).  이때 토픽의 수는 하이퍼 파라미터로, 사용자가 직접 지정해줘야한다. 

LDA의 순서는 아래와 같다.
1. 단순 Count 기반 Document Term Matrix 행렬을 생성한다.(문서별 단어의 빈도수 행렬)
2. 사용자가 토픽의 개수를 사전에 설정한다.(하이퍼파라미터)
3. 각 단어들을 임의의 토픽으로 최초 할당하여 문서별 토픽 분포와 토픽별 단어 분포를 결정한다.
4. 특정 단어를 하나 추출하고 추출한 해당 단어를 제외하고 문서의 토픽 분포와 토픽별  단어 분포를 다시 계산한다.(이 과정을 깁스 샘플링이라고 한다.) 그리고 추출된 단어는 새롭게 토픽 할당 분포를 계산한다.
5. 모든 단어에 대해 4단계를 반복한다.
6. 모든 단어들의 토픽 할당 분포가 변경되지 않고 수렴할 때까지 수행한다.

**[유용한 자료]**
- [[잠재 디리클레 할당 파헤치기] 3. 깁스 샘플링으로 파라미터 추정](https://bab2min.tistory.com/569)

</br>

참고 문헌
- [[NLP] LDA를 활용한 Topic Modeling 구현하기](https://techblog-history-younghunjo1.tistory.com/112)
- [LDA 토픽모델링](https://www.youtube.com/watch?v=noWKlkdcY6A)
- [위키백과 - 디리클레 분포](https://ko.wikipedia.org/wiki/%EB%94%94%EB%A6%AC%ED%81%B4%EB%A0%88_%EB%B6%84%ED%8F%AC)
- [Topic Modeling, LDA](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/06/01/LDA/)

---
