<div align='center'>
  <h1>🤖 Machine Learning 🤖</h1>
</div>


## Questions ❓

- [알고 있는 metric에 대해 설명해주세요. (ex. RMSE, MAE, recall, precision ...)](#no1)
- [정규화를 왜 해야할까요? 정규화의 방법은 무엇이 있나요?](#no2)
- [Local Minima와 Global Minima에 대해 설명해주세요.](#no3)
- [차원의 저주에 대해 설명해주세요.](#no4)
- [dimension reduction기법으로 보통 어떤 것들이 있나요?](#no5)
- [PCA는 차원 축소 기법이면서, 데이터 압축 기법이기도 하고, 노이즈 제거기법이기도 합니다. 왜 그런지 설명해주실 수 있나요?](#no6)

---

### No.1

**알고 있는 metric에 대해 설명해주세요. (ex. RMSE, MAE, recall, precision ...)**

1. RMSE-like metrics
   * [MAE](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanAbsoluteError): Mean Absolute Error, 오차 평균; 오차의 단위가 실제 타겟값의 단위와 똑같다는 특징이 있으며 이상치에 덜 민감하다.
   * [RMSE](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/RootMeanSquaredError): Root Mean Squared Error, 오차 제곱 평균의 제곱근; MAE와 마찬가지로 오차의 단위가 실제 타겟값의 단위와 똑같아서 직관적이다. MAE와는 달리 큰 오차값이 더 반영되어 이상치에 더 민감하다는 특징을 갖고 있다.
   * [MSE](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanSquaredError): Mean Squared Error, 오차 제곱 평균; RMSE와 같이 큰 오차값을 더 반영하는 평가지표지만, 실제 타겟값과 단위가 달라서 직관적이지는 않다. MAE, RMSE와 같이 회귀 문제에서 자주 사용되는 평가지표다.
   * [MAPE](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanAbsolutePercentageError): Mean Absolute Percentage Error, 오차 평균 백분율; 오차를 실제값 y로 나누어 MAE를 백분율 단위로 변경한 것으로, MAE와 많은 공통점을 지니지만 y가 0일 때 사용할 수 없다는 단점이 추가로 존재한다. 백분율 단위의 직관성이 장점이지만, 예측값과 실제값의 대소관계가 지표의 크기에 반영이 되는 특징이 있어 주의가 필요하다. (예/ n=1일 때, 예측값:10/실제값:20 → MAPE = 50% ; 예측값:20/실제값:10 → MAPE = 100%)
2. confusion matrix related metrics
   * [Precision](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision)(정밀도): True Positives / Predicted Positives = True Positives / (True Positives + False Positives): 1종 오류와 관련이 있는 분류 평가 지표. (예/ 스팸 방지 - 스팸메일이 받은메일함으로 분류되는 것보다 정상 메일이 스팸메일함으로 분류되는 것을 더 지양)
   * [Recall](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall) = Sensitivity(민감도): True Positives / Real Positives = True Positives / (True Positives + False Negatives); 2종 오류와 관련이 있는 분류 평가 지표. (예/ 의료 데이터 분석 - 양성으로 잘못 분류하는 것보다 음성으로 잘못 분류하여 환자를 놓치는 경우를 더 지양)
   * Specificity(특이도): True Negatives / Real Negatives = True Negatives / (True Negatives + False Positives); False Alarm(False Positive)를 방지하고자 할때 사용. (예/ 마약복용자를 구속하지 않는 것보다 일반인을 잘못 구속하는 경우를 더 지양)
   * [Precision at Recall](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/PrecisionAtRecall) / [Recall at Precision](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/RecallAtPrecision) : 특정 Recall 값에서의 Precision 측정 또는 특정 Precision 값에서의 Recall 측정
3. [AUC](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/AUC)
   * ROC-AUC: ROC(Receiver Operating Characteristic) 곡선 아래의 면적(0~1); True Positive Rate(=Recall=TP/(TP+FN))과 False Positive Rate(=FP/(FP+TN))의 tradeoff 관계를 시각화하는 지표; 랜덤한 이진 분류의 경우 ROC-AUC 값은 0.5.
   * PR-AUC: Precision-Recall 곡선 아래의 면적(0~1); 음악 장르와 같이 대부분이 0으로 이루어진 (imbalanced 한) 레이블 벡터 y를 타겟으로 할 때는 PR-AUC가 ROC-AUC보다 더 정확한 직관을 주는 것으로 알려져 있음. (True Negative의 영향 때문)
4. crossentropy
   * [binary crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/BinaryCrossentropy): 이진분류 문제에 사용할 수 있는 crossentropy 기반 분류 지표.
   * [categorical crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/CategoricalCrossentropy): multi-label분류 문제에 사용할 수 있는 crossentropy 기반 분류 지표.

참고문헌
- [TensorFlow tf.keras.metrics API docs](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)
- [Regression Metrics for Machine Learning](https://machinelearningmastery.com/regression-metrics-for-machine-learning/)
- [Tutorial: Understanding Regression Error Metrics in Python](https://www.dataquest.io/blog/understanding-regression-error-metrics/)
- [Accuracy, Recall, Precision, F-Score & Specificity, which to optimize on?](https://towardsdatascience.com/accuracy-recall-precision-f-score-specificity-which-to-optimize-on-867d3f11124)
- [F1 Score vs ROC AUC vs Accuracy vs PR AUC: Which Evaluation Metric Should You Choose?](https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc)
- [이진 분류기 성능 평가방법 AUC(area under the ROC curve)의 이해.txt](https://bskyvision.com/1165)
- [Cross-entropy for classification](https://towardsdatascience.com/cross-entropy-for-classification-d98e7f974451)

---
### No.2
**정규화를 왜 해야할까요? 정규화의 방법은 무엇이 있나요?**

다양한 정규화 방법들을 통해서 기계학습 알고리즘의 성능을 증가시킬 수 있으며, 알고리즘들마다 정규화의 효과가 다를 수 있어 실험을 해보는 것이 좋다. 거리기반의 SVM, KNN 알고리즘과 MLP 알고리즘은 특히 정규화의 효능이 좋은 편이다.

1. Normalization (min-max normalization): 값들을 0에서 1사이 범위로 rescaling하는 것을 뜻한다.
2. Standardization (Z-score standardization): 평균 0 표준편차 1의 분포를 띄도록 rescaling하는 것을 뜻한다.

특히 딥러닝에서 정규화는 오랜 기간동안 연구 대상으로 주목 받고 있었으며, 정규화를 수행하는 방향 또한 중요하다고 알려져 있다.

![normalization](https://miro.medium.com/max/1400/1*r0HM4TvZvvceXcJIpDJmDQ.png)

1. Batch Normalization: mini-batch 내 같은 feature들끼리 정규화를 수행하는 방법이다. 보통 CNN 모델에서 많이 사용된다. batch 사이자가 너무 작을 때는 오히려 악역향을 끼칠 수 있다고도 한다.
2. Layer Normalization: 각 데이터 포인트 전체에 대하여 정규화를 수행하는 방법이다. 보통 RNN 또는 Transformer 모델에서 많이 사용된다.

  참고문헌
  - [Normalization vs Standardization — Quantitative analysis](https://towardsdatascience.com/normalization-vs-standardization-quantitative-analysis-a91e8a79cebf)
  - [Normalization Techniques in Deep Neural Networks](https://medium.com/techspace-usict/normalization-techniques-in-deep-neural-networks-9121bf100d8)

---

### No.3
**Local Minima와 Global Minima에 대해 설명해주세요.**

(손실) 함수 전체의 가장 최소값이 global minima이며, gradient decent 등의 최적화 알고리즘으로 찾을 수 있는 global minima 외 최소값들을 local minima라고 한다. 기계학습 최적화 알고리즘들은 local minima에 빠져서 global minima를 찾지 못할 가능성이 항상 존재한다.

![animation](https://vitalflux.com/wp-content/uploads/2020/10/local_minima_vs_global_minima.gif)

global minima를 올바르게 찾기 위해서는 feature engineering을 조심히 히야하며, 다양한 learning rate schedule과 여러 step수를 실험해볼 필요가 있다.

참고문헌

- [Local & Global Minima Explained with Examples](https://vitalflux.com/local-global-maxima-minima-explained-examples/)
---

### No.4
**차원의 저주에 대해 설명해주세요.**

`차원의 저주`란 데이터 학습을 위해 차원이 증가(=변수의 수 증가)하면서 학습 데이터의 수가 차원의 수보다 적어져 성능이 저하되는 현상을 말한다. 즉, 간단히 말하면 차원이 증가함에 따라 모델의 성능이 안 좋아지는 현상을 의미한다.

무조건 변수의 수가 증가한다고 해서 차원의 저주 문제가 있는 것이 아니라, `관측치 수보다 변수의 수가 많아지면` 발생한다. 예를 들어, 관측치의 수는 200개인데, 변수는 7000개인 경우 차원의 저주가 발생하게 된다.

<img src="https://user-images.githubusercontent.com/57162812/150798251-f38d8347-b146-44ee-9056-15194951a7d3.png" width=600>

차원이 늘어날 때마다 점들 사이의 간격이 벌어지게 된다. 즉, 빈 공간이 생기게 되는데 이는 컴퓨터 상에서 0으로 채워졌다는 뜻으로 정보가 없는 셈이다. 정보가 적기 때문에 모델을 돌릴 때 성능이 저하되게 된다. 여기서 빈 공간이 생기는 것을 차원의 저주라고 한다.

차원의 저주를 해결할 수 있는 방법에는 차원을 축소시키거나 데이터를 많이 획득하는 것이 있다.

참고문헌
- [[빅데이터] 차원의 저주(The curse of dimensionality)](https://datapedia.tistory.com/15)

---

### No.5
**dimension reduction기법으로 보통 어떤 것들이 있나요?**

`차원 축소` : 매우 많은 피처로 구성된 다차원 데이터 세트의 타원을 축소해 새로운 차원의 데이터 세트를 생성하는 것

차원 축소는 **피처 선택**과 **피처 추출**로 나눌 수 있다.

`피처 선택`은 특정 피처에 종속성이 강한 불필요한 피처는 아예 제거하는 것이며, `피처 추출`은 기존 피처를 저차원의 중요 피처로 압축해서 추출하는 것이다.

피쳐 선택의 장점은 선택한 피처의 해석이 용이하다는 점이고, 단점은 피처간의 상관관계를 고려하기 어렵다는 점이다.

피처 추출의 장점은 피처 간 상관관계를 고려하기 용이하며 피처의 개수를 많이 줄일 수 있다는 점이고, 단점은 추출된 변수의 해석이 어렵다는 점이다. 대표적인 피처 추출 알고리즘으로는 PCA, SVD, NMF, LDA 등이 있다.

참고 문헌
- [차원 축소 (Dimension Reduction) - PCA, LDA](https://casa-de-feel.tistory.com/19)

---
### No.6
**PCA는 차원 축소 기법이면서, 데이터 압축 기법이기도 하고, 노이즈 제거기법이기도 합니다. 왜 그런지 설명해주실 수 있나요?**

주성분 분석은 차원이 큰 벡터에서 선형 독립하는 고유 벡터만을 남겨두고 차원을 축소하게 된다. 이때 상관성이 높은 독립 변수들을 N개의 선형 조합으로 만들며 개수를 요약, 압축해내는 기법이다. 그리고 이 압축된 각각의 돌깁 변수들은 선형 독립, 즉 직교하며 낮은 상관성을 보이게 된다. 또한 정보 설명력이 높은 주성분들만 선택하면서 정보 설명력이 낮은, 노이즈로 구성된 칼럼들을 배제하기 때문에 노이즈 제거 기법이라고도 불린다.

참고 문헌
- [[기술면접] 차원축소, PCA, SVD, LSA, LDA, MF 간단정리 (day1 / 201009) - Hui_dea](https://huidea.tistory.com/126)
