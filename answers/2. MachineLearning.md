<div align='center'>
  <h1>ğŸ¤– Machine Learning ğŸ¤–</h1>
</div>


## Questions â“

- [ì•Œê³  ìˆëŠ” metricì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”. (ex. RMSE, MAE, recall, precision ...)](#no1)
- [ì •ê·œí™”ë¥¼ ì™œ í•´ì•¼í• ê¹Œìš”? ì •ê·œí™”ì˜ ë°©ë²•ì€ ë¬´ì—‡ì´ ìˆë‚˜ìš”?](#no2)
- [Local Minimaì™€ Global Minimaì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.](#no3)

---

### No.1

**ì•Œê³  ìˆëŠ” metricì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”. (ex. RMSE, MAE, recall, precision ...)**

1. RMSE-like metrics
   * [MAE](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanAbsoluteError): Mean Absolute Error, ì˜¤ì°¨ í‰ê· ; ì˜¤ì°¨ì˜ ë‹¨ìœ„ê°€ ì‹¤ì œ íƒ€ê²Ÿê°’ì˜ ë‹¨ìœ„ì™€ ë˜‘ê°™ë‹¤ëŠ” íŠ¹ì§•ì´ ìˆìœ¼ë©° ì´ìƒì¹˜ì— ëœ ë¯¼ê°í•˜ë‹¤.
   * [RMSE](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/RootMeanSquaredError): Root Mean Squared Error, ì˜¤ì°¨ ì œê³± í‰ê· ì˜ ì œê³±ê·¼; MAEì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì˜¤ì°¨ì˜ ë‹¨ìœ„ê°€ ì‹¤ì œ íƒ€ê²Ÿê°’ì˜ ë‹¨ìœ„ì™€ ë˜‘ê°™ì•„ì„œ ì§ê´€ì ì´ë‹¤. MAEì™€ëŠ” ë‹¬ë¦¬ í° ì˜¤ì°¨ê°’ì´ ë” ë°˜ì˜ë˜ì–´ ì´ìƒì¹˜ì— ë” ë¯¼ê°í•˜ë‹¤ëŠ” íŠ¹ì§•ì„ ê°–ê³  ìˆë‹¤.
   * [MSE](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanSquaredError): Mean Squared Error, ì˜¤ì°¨ ì œê³± í‰ê· ; RMSEì™€ ê°™ì´ í° ì˜¤ì°¨ê°’ì„ ë” ë°˜ì˜í•˜ëŠ” í‰ê°€ì§€í‘œì§€ë§Œ, ì‹¤ì œ íƒ€ê²Ÿê°’ê³¼ ë‹¨ìœ„ê°€ ë‹¬ë¼ì„œ ì§ê´€ì ì´ì§€ëŠ” ì•Šë‹¤. MAE, RMSEì™€ ê°™ì´ íšŒê·€ ë¬¸ì œì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” í‰ê°€ì§€í‘œë‹¤.
   * [MAPE](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanAbsolutePercentageError): Mean Absolute Percentage Error, ì˜¤ì°¨ í‰ê·  ë°±ë¶„ìœ¨; ì˜¤ì°¨ë¥¼ ì‹¤ì œê°’ yë¡œ ë‚˜ëˆ„ì–´ MAEë¥¼ ë°±ë¶„ìœ¨ ë‹¨ìœ„ë¡œ ë³€ê²½í•œ ê²ƒìœ¼ë¡œ, MAEì™€ ë§ì€ ê³µí†µì ì„ ì§€ë‹ˆì§€ë§Œ yê°€ 0ì¼ ë•Œ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤ëŠ” ë‹¨ì ì´ ì¶”ê°€ë¡œ ì¡´ì¬í•œë‹¤. ë°±ë¶„ìœ¨ ë‹¨ìœ„ì˜ ì§ê´€ì„±ì´ ì¥ì ì´ì§€ë§Œ, ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ëŒ€ì†Œê´€ê³„ê°€ ì§€í‘œì˜ í¬ê¸°ì— ë°˜ì˜ì´ ë˜ëŠ” íŠ¹ì§•ì´ ìˆì–´ ì£¼ì˜ê°€ í•„ìš”í•˜ë‹¤. (ì˜ˆ/ n=1ì¼ ë•Œ, ì˜ˆì¸¡ê°’:10/ì‹¤ì œê°’:20 â†’ MAPE = 50% ; ì˜ˆì¸¡ê°’:20/ì‹¤ì œê°’:10 â†’ MAPE = 100%)
2. confusion matrix related metrics
   * [Precision](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision)(ì •ë°€ë„): True Positives / Predicted Positives = True Positives / (True Positives + False Positives): 1ì¢… ì˜¤ë¥˜ì™€ ê´€ë ¨ì´ ìˆëŠ” ë¶„ë¥˜ í‰ê°€ ì§€í‘œ. (ì˜ˆ/ ìŠ¤íŒ¸ ë°©ì§€ - ìŠ¤íŒ¸ë©”ì¼ì´ ë°›ì€ë©”ì¼í•¨ìœ¼ë¡œ ë¶„ë¥˜ë˜ëŠ” ê²ƒë³´ë‹¤ ì •ìƒ ë©”ì¼ì´ ìŠ¤íŒ¸ë©”ì¼í•¨ìœ¼ë¡œ ë¶„ë¥˜ë˜ëŠ” ê²ƒì„ ë” ì§€ì–‘)
   * [Recall](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall) = Sensitivity(ë¯¼ê°ë„): True Positives / Real Positives = True Positives / (True Positives + False Negatives); 2ì¢… ì˜¤ë¥˜ì™€ ê´€ë ¨ì´ ìˆëŠ” ë¶„ë¥˜ í‰ê°€ ì§€í‘œ. (ì˜ˆ/ ì˜ë£Œ ë°ì´í„° ë¶„ì„ - ì–‘ì„±ìœ¼ë¡œ ì˜ëª» ë¶„ë¥˜í•˜ëŠ” ê²ƒë³´ë‹¤ ìŒì„±ìœ¼ë¡œ ì˜ëª» ë¶„ë¥˜í•˜ì—¬ í™˜ìë¥¼ ë†“ì¹˜ëŠ” ê²½ìš°ë¥¼ ë” ì§€ì–‘)
   * Specificity(íŠ¹ì´ë„): True Negatives / Real Negatives = True Negatives / (True Negatives + False Positives); False Alarm(False Positive)ë¥¼ ë°©ì§€í•˜ê³ ì í• ë•Œ ì‚¬ìš©. (ì˜ˆ/ ë§ˆì•½ë³µìš©ìë¥¼ êµ¬ì†í•˜ì§€ ì•ŠëŠ” ê²ƒë³´ë‹¤ ì¼ë°˜ì¸ì„ ì˜ëª» êµ¬ì†í•˜ëŠ” ê²½ìš°ë¥¼ ë” ì§€ì–‘)
   * [Precision at Recall](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/PrecisionAtRecall) / [Recall at Precision](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/RecallAtPrecision) : íŠ¹ì • Recall ê°’ì—ì„œì˜ Precision ì¸¡ì • ë˜ëŠ” íŠ¹ì • Precision ê°’ì—ì„œì˜ Recall ì¸¡ì •
3. [AUC](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/AUC)
   * ROC-AUC: ROC(Receiver Operating Characteristic) ê³¡ì„  ì•„ë˜ì˜ ë©´ì (0~1); True Positive Rate(=Recall=TP/(TP+FN))ê³¼ False Positive Rate(=FP/(FP+TN))ì˜ tradeoff ê´€ê³„ë¥¼ ì‹œê°í™”í•˜ëŠ” ì§€í‘œ; ëœë¤í•œ ì´ì§„ ë¶„ë¥˜ì˜ ê²½ìš° ROC-AUC ê°’ì€ 0.5.
   * PR-AUC: Precision-Recall ê³¡ì„  ì•„ë˜ì˜ ë©´ì (0~1); ìŒì•… ì¥ë¥´ì™€ ê°™ì´ ëŒ€ë¶€ë¶„ì´ 0ìœ¼ë¡œ ì´ë£¨ì–´ì§„ (imbalanced í•œ) ë ˆì´ë¸” ë²¡í„° yë¥¼ íƒ€ê²Ÿìœ¼ë¡œ í•  ë•ŒëŠ” PR-AUCê°€ ROC-AUCë³´ë‹¤ ë” ì •í™•í•œ ì§ê´€ì„ ì£¼ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆìŒ. (True Negativeì˜ ì˜í–¥ ë•Œë¬¸)
4. crossentropy
   * [binary crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/BinaryCrossentropy): ì´ì§„ë¶„ë¥˜ ë¬¸ì œì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” crossentropy ê¸°ë°˜ ë¶„ë¥˜ ì§€í‘œ.
   * [categorical crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/CategoricalCrossentropy): multi-labelë¶„ë¥˜ ë¬¸ì œì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” crossentropy ê¸°ë°˜ ë¶„ë¥˜ ì§€í‘œ.

ì°¸ê³ ë¬¸í—Œ
- [TensorFlow tf.keras.metrics API docs](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)
- [Regression Metrics for Machine Learning](https://machinelearningmastery.com/regression-metrics-for-machine-learning/)
- [Tutorial: Understanding Regression Error Metrics in Python](https://www.dataquest.io/blog/understanding-regression-error-metrics/)
- [Accuracy, Recall, Precision, F-Score & Specificity, which to optimize on?](https://towardsdatascience.com/accuracy-recall-precision-f-score-specificity-which-to-optimize-on-867d3f11124)
- [F1 Score vs ROC AUC vs Accuracy vs PR AUC: Which Evaluation Metric Should You Choose?](https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc)
- [ì´ì§„ ë¶„ë¥˜ê¸° ì„±ëŠ¥ í‰ê°€ë°©ë²• AUC(area under the ROC curve)ì˜ ì´í•´.txt](https://bskyvision.com/1165)
- [Cross-entropy for classification](https://towardsdatascience.com/cross-entropy-for-classification-d98e7f974451)

---
### No.2
**ì •ê·œí™”ë¥¼ ì™œ í•´ì•¼í• ê¹Œìš”? ì •ê·œí™”ì˜ ë°©ë²•ì€ ë¬´ì—‡ì´ ìˆë‚˜ìš”?**

ë‹¤ì–‘í•œ ì •ê·œí™” ë°©ë²•ë“¤ì„ í†µí•´ì„œ ê¸°ê³„í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ì„ ì¦ê°€ì‹œí‚¬ ìˆ˜ ìˆìœ¼ë©°, ì•Œê³ ë¦¬ì¦˜ë“¤ë§ˆë‹¤ ì •ê·œí™”ì˜ íš¨ê³¼ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ ì‹¤í—˜ì„ í•´ë³´ëŠ” ê²ƒì´ ì¢‹ë‹¤. ê±°ë¦¬ê¸°ë°˜ì˜ SVM, KNN ì•Œê³ ë¦¬ì¦˜ê³¼ MLP ì•Œê³ ë¦¬ì¦˜ì€ íŠ¹íˆ ì •ê·œí™”ì˜ íš¨ëŠ¥ì´ ì¢‹ì€ í¸ì´ë‹¤.

1. Normalization (min-max normalization): ê°’ë“¤ì„ 0ì—ì„œ 1ì‚¬ì´ ë²”ìœ„ë¡œ rescalingí•˜ëŠ” ê²ƒì„ ëœ»í•œë‹¤.
2. Standardization (Z-score standardization): í‰ê·  0 í‘œì¤€í¸ì°¨ 1ì˜ ë¶„í¬ë¥¼ ë„ë„ë¡ rescalingí•˜ëŠ” ê²ƒì„ ëœ»í•œë‹¤.

íŠ¹íˆ ë”¥ëŸ¬ë‹ì—ì„œ ì •ê·œí™”ëŠ” ì˜¤ëœ ê¸°ê°„ë™ì•ˆ ì—°êµ¬ ëŒ€ìƒìœ¼ë¡œ ì£¼ëª© ë°›ê³  ìˆì—ˆìœ¼ë©°, ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë°©í–¥ ë˜í•œ ì¤‘ìš”í•˜ë‹¤ê³  ì•Œë ¤ì ¸ ìˆë‹¤.

![normalization](https://miro.medium.com/max/1400/1*r0HM4TvZvvceXcJIpDJmDQ.png)

1. Batch Normalization: mini-batch ë‚´ ê°™ì€ featureë“¤ë¼ë¦¬ ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì´ë‹¤. ë³´í†µ CNN ëª¨ë¸ì—ì„œ ë§ì´ ì‚¬ìš©ëœë‹¤. batch ì‚¬ì´ìê°€ ë„ˆë¬´ ì‘ì„ ë•ŒëŠ” ì˜¤íˆë ¤ ì•…ì—­í–¥ì„ ë¼ì¹  ìˆ˜ ìˆë‹¤ê³ ë„ í•œë‹¤.
2. Layer Normalization: ê° ë°ì´í„° í¬ì¸íŠ¸ ì „ì²´ì— ëŒ€í•˜ì—¬ ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì´ë‹¤. ë³´í†µ RNN ë˜ëŠ” Transformer ëª¨ë¸ì—ì„œ ë§ì´ ì‚¬ìš©ëœë‹¤.

  ì°¸ê³ ë¬¸í—Œ
  - [Normalization vs Standardization â€” Quantitative analysis](https://towardsdatascience.com/normalization-vs-standardization-quantitative-analysis-a91e8a79cebf)
  - [Normalization Techniques in Deep Neural Networks](https://medium.com/techspace-usict/normalization-techniques-in-deep-neural-networks-9121bf100d8)

---

### No.3
**Local Minimaì™€ Global Minimaì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.**

(ì†ì‹¤) í•¨ìˆ˜ ì „ì²´ì˜ ê°€ì¥ ìµœì†Œê°’ì´ global minimaì´ë©°, gradient decent ë“±ì˜ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì°¾ì„ ìˆ˜ ìˆëŠ” global minima ì™¸ ìµœì†Œê°’ë“¤ì„ local minimaë¼ê³  í•œë‹¤. ê¸°ê³„í•™ìŠµ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ë“¤ì€ local minimaì— ë¹ ì ¸ì„œ global minimaë¥¼ ì°¾ì§€ ëª»í•  ê°€ëŠ¥ì„±ì´ í•­ìƒ ì¡´ì¬í•œë‹¤.

![animation](https://vitalflux.com/wp-content/uploads/2020/10/local_minima_vs_global_minima.gif)

global minimaë¥¼ ì˜¬ë°”ë¥´ê²Œ ì°¾ê¸° ìœ„í•´ì„œëŠ” feature engineeringì„ ì¡°ì‹¬íˆ íˆì•¼í•˜ë©°, ë‹¤ì–‘í•œ learning rate scheduleê³¼ ì—¬ëŸ¬ stepìˆ˜ë¥¼ ì‹¤í—˜í•´ë³¼ í•„ìš”ê°€ ìˆë‹¤.

ì°¸ê³ ë¬¸í—Œ

- [Local & Global Minima Explained with Examples](https://vitalflux.com/local-global-maxima-minima-explained-examples/)
---

