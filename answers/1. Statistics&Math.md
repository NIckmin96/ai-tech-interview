<div align='center'>
  <h1>📈 Statistics/Math 📈</h1>
</div>

## Questions ❓

- [고유값(eigen value)와 고유벡터(eigen vector)에 대해 설명해주세요. 그리고 왜 중요할까요?](#no1)
- [샘플링(Sampling)과 리샘플링(Resampling)에 대해 설명해주세요. 리샘플링은 무슨 장점이 있을까요?](#no2)
- [확률 모형과 확률 변수는 무엇일까요?](#no3)
- [누적 분포 함수와 확률 밀도 함수는 무엇일까요? 수식과 함께 표현해주세요.](#no4)
- [조건부 확률은 무엇일까요?](#no5)
- [공분산과 상관계수는 무엇일까요? 수식과 함께 표현해주세요.](#no6)
- [신뢰 구간의 정의는 무엇인가요?](#no7)
- [p-value를 모르는 사람에게 설명한다면 어떻게 설명하실 건가요?](#no8)
- [R square의 의미는 무엇인가요?](#no9)
- [평균(mean)과 중앙값(median)중에 어떤 케이스에서 뭐를 써야할까요?](#no10)
- [중심극한정리는 왜 유용한걸까요?](#no11)
- [엔트로피(entropy)에 대해 설명해주세요. 가능하면 Information Gain도요.](#no12)
- [어떨 때 모수적 방법론을 쓸 수 있고, 어떨 때 비모수적 방법론을 쓸 수 있나요?](#no13)
- [“likelihood”와 “probability”의 차이는 무엇일까요?](#no14)
- [통계에서 사용되는 bootstrap의 의미는 무엇인가요.](#no15)
- [데이터가 매우 적은 (수십개 이하) 케이스의 경우 어떤 방식으로 예측 모델을 수립할 수 있을까요?](#no16)
- [베이지안과 프리퀀티스트 간의 입장차이를 설명해주실 수 있나요?](#no17)
- [검정력(statistical power)은 무엇일까요?](#no18)
- [missing value가 있을 경우 채워야 할까요? 그 이유는 무엇인가요?](#no19)
- [아웃라이어의 판단하는 기준은 무엇인가요?](#no20)
- [필요한 표본의 크기를 어떻게 계산합니까?](#no21)
- [Bias를 통제하는 방법은 무엇입니까?](#no22)
- [로그 함수는 어떤 경우 유용합니까? 사례를 들어 설명해주세요.](#no23)
- [베르누이 분포 / 이항 분포 / 카테고리 분포 / 다항 분포 / 가우시안 정규 분포 / t 분포 / 카이제곱 분포 / F 분포 / 베타 분포 / 감마 분포에 대해 설명해주세요. 그리고 분포 간의 연관성도 설명해주세요.](#no24)
- [출장을 위해 비행기를 타려고 합니다. 당신은 우산을 가져가야 하는지 알고 싶어 출장지에 사는 친구 3명에게 무작위로 전화를 하고 비가 오는 경우를 독립적으로 질문해주세요. 각 친구는 2/3로 진실을 말하고 1/3으로 거짓을 말합니다. 3명의 친구가 모두 “그렇습니다. 비가 내리고 있습니다”라고 말했습니다. 실제로 비가 내릴 확률은 얼마입니까?](#no25)

---

### No.1

**고유값(eigen value)와 고유벡터(eigen vector)에 대해 설명해주세요. 그리고 왜 중요할까요?**

행렬은 선형 변환 연산이다. 즉, 행렬은 벡터를 변환시켜 다른 벡터를 출력해준다.

![image](https://user-images.githubusercontent.com/57162812/149796947-1e2c1562-eec6-49a3-8d66-d2f87fdfc5ee.png)

그런데, 특정한 벡터와 행렬은 선형 변환을 취해주었을 때, 크기만 바뀌고 방향은 바뀌지 않을 수도 있다. 즉, 입력 벡터 x를 A로 선형변환 시킨 결과(Ax)가 상수배라는 것이다.

> 정의 : 임의의 n x n 행렬 A에 대하여, 다음 식을 만족하는 영벡터가 아닌 벡터 x와 λ가 존재한다. 이 때, x를 고유벡터(eigen vector), λ를 고유값(eigen value)라고 한다.  
>   
>  <p align="center"><img src="https://latex.codecogs.com/svg.latex?\Large&space;A\overrightarrow{x}=\lambda\overrightarrow{x}" />

고유값, 고유벡터 그 자체의 활용보다는 `SVD(특이값 분해)`, `Pseudo-Inverse`, `선형연립방정식의 풀이`, `PCA(주성분 분석)` 등의 주용 응용이 고유값과 고유벡터를 밑바탕으로 하고 있기 때문에 중요하다.  

참고문헌
- [고윳값과 고유벡터 - 공돌이의 수학정리노트](https://angeloyeo.github.io/2019/07/17/eigen_vector.html)
- [[선형대수학 #3] 고유값과 고유벡터 (eigenvalue & eigenvector)](https://darkpgmr.tistory.com/105)

---
### No.2
**샘플링(Sampling)과 리샘플링(Resampling)에 대해 설명해주세요. 리샘플링은 무슨 장점이 있을까요?**

`샘플링(Sampling)` : 모집단에서 임의의 Sampling을 뽑아내는 것이다. 즉, 표본 추출이라 할 수 있다. 표본은 모집단을 닮은 존재이지만, 모집단 그 자체는 아니기 때문에 표본에는 반드시 모집단의 원래 패턴을 놓치는 부분이 존재할 수 밖에 없다.
  1. 단순무작위추출 : 무작위로 추출하는 방법
  2. 층화추출 : 모집단을 구성하고 있는 구성요소들이 자연적인 순서 또는 일정한 질서에 따라 배열된 목록에서 매 k번째의 구성요소를 추출하여 형성한 추출
  3. 계통추출 : 서로 인접한 조사 단위들을 묶어 군집으로 구분하고, 일부의 군집을 추출한 모든 자료를 활용하거나 샘플링하는 방법

`리샘플링(Resampling)` : 가지고 있는 샘플에서 다시 샘플 부분집합을 뽑아서 통계량의 변동성을 확인하는 것이다. 즉, 같은 샘플을 여러 번 사용해서 성능을 측정하는 방식이다.
  1. K-fold Cross Validation : k-1 개의 부분집합을 훈련 세트로 사용하고 남은 하나의 부분집합을 검증 데이터로 사용
  2. 부트스트랩 : 표본에서 추가적으로 표본을 복원 추출하고 각 표본에 대한 통계량을 다시 계산하는 것

 리샘플링의 잠점
  1. 모집단의 어떤 가정도 필요 없이 표본만으로 추론이 가능하다.
  2. 통계학적 표본 분포에 대한 지식이 필요 없다.

  참고문헌
  - [샘플링과 리샘플링의 차이는 무엇일까? | 김감귤](https://kejdev.github.io/posts/sampling-resampling/)
  - [(데이터과학 인터뷰 질문)(2) 샘플링과 리샘플링, 1편](https://cnp-0717.tistory.com/7)
  - [표본 추출 방법, 확률 표본 추출의 종류 - 자비스가 필요해](https://needjarvis.tistory.com/475)
  - [DATA - 12. 부트스트랩(Bootstrap) - 귀퉁이 서재](https://bkshin.tistory.com/entry/DATA-12)
  - [샘플링과 리샘플링 - Wriggling](https://trampled-worm.tistory.com/91)

---

### No.3
**확률 모형과 확률 변수는 무엇일까요?**

`확률 변수` : 확률로 표현하기 위한 event를 정의한 것이다. 어떤 것을 확률로 표현할 것인지에 대해 다양하게 정의가 가능하므로 변수라는 용어를 사용한다. 그리고 확률이 정의된 Sample Space 내에서, 이러한 확률변수를 0과 1 사이의 확률로 mapping하는 함수를 **확률 함수(확률 분포 함수)** 라고 한다. 확률 변수는 **이산확률변수(discrete random variable)** 과 **연속확률변수(continuous random variable)** 로 구분된다.
  1. 이산 확률 변수 : 확률 변수가 취할 수 있는 값이 유한하거나 무한하더라도 하나씩 셀 수 있는 경우
  2. 연속 확률 변수 : 확률 변수가 연속적인 구간의 값을 취하는 경우

`확률 모형` : 데이터와 확률간의 관계 즉, 확률 변수를 이용한 데이터 분포를 수학적으로 정의하는 방법을 말한다. 보통 미리 정해진 확률함수의 수식을 사용하며 이 함수들의 계수를 분포의 **모수**라고 부른다.

참고문헌
- [확률변수와 확률모형 - 숨니의 무작정 따라하기](https://sumniya.tistory.com/24)
- [확률 모형과 확률 변수 - velog](https://velog.io/@ohs2251/%ED%99%95%EB%A5%A0-%EB%AA%A8%ED%98%95%EA%B3%BC-%ED%99%95%EB%A5%A0-%EB%B3%80%EC%88%98)
- [[ADsP] 확률 변수를 정리했다. (이산형, 연속형) - 네이버 블로그](https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=tjgml1343&logNo=221979604865)
  
---

### No.4
**누적 분포 함수와 확률 밀도 함수는 무엇일까요? 수식과 함께 표현해주세요.**

`확률 밀도 함수(Probability Density Function, PDF)` : 연속확률변수의 확률 분포를 나타내는 함수로 다음의 식들을 만족하는 함수이다.
  1. 모든 xϵR에 대하여 <img src="https://latex.codecogs.com/svg.latex?\small&space;f(x)\geq0" />
  2. <img src="https://latex.codecogs.com/svg.latex?\small&space;\int^\infty_{-\infty}f(x)dx=1" />
  3. <img src="https://latex.codecogs.com/svg.latex?\small&space;P(a\leq" /><img src="https://latex.codecogs.com/svg.latex?\small&space;X\leq" /><img src="https://latex.codecogs.com/svg.latex?\small&space;b)" /><img src="https://latex.codecogs.com/svg.latex?\small&space;=\int^b_af(x)dx" />

`누적 분포 함수(Cumulative Distaribution Function, CDF)` : 랜덤 변수 X에 대하여 확률이 정의되면, 다음과 같이 정의되는 함수 F(x)를 X의 누적 분포 함수
  <!-- $$
F(a) = P(X ≤ a) = \int^a_{-\infty} f(x) dx
$$ -->

<div align="center"><img style="background: white;" src="https://render.githubusercontent.com/render/math?math=F(a)%20%3D%20P(X%20%E2%89%A4%20a)%20%3D%20%5Cint%5Ea_%7B-%5Cinfty%7D%20f(x)%20dx%0D" height=25></div>

확률 밀도 함수와 누적 분포 함수는 **미분과 적분의 관계**를 갖는다는 것을 알 수 있다.

 참고문헌
  - [[확률과 통계] 18. 확률밀도함수, Probability Density Function of ...](https://m.blog.naver.com/mykepzzang/220835810657)
  - [[통계학] 1.5 누적 분포 함수 Cumulative Distribution Functions](https://elementary-physics.tistory.com/127)

---

### No.5
**조건부 확률은 무엇일까요?**

`조건부 확률` : 특정 사건이 일어났을 때, 연달아 다른 사건이 일어나느 것을 확률로 나타낸 것이다. 즉, 사건 A와 사건 B를 예로들면 A가 일어났을 때 B가 일어날 확률을 구하는 것이 조건부 확률이 된다. 조건부 확률은 다음과 같이 표현된다.

<p align="center"><img src='https://user-images.githubusercontent.com/57162812/149852129-10079690-6310-48f2-8908-1d14a6440b2b.png' width="150" height="60"/>

조건부 확률을 통해서 사후 확률, 그리고 베이즈정리를 표현할 수 있다.

`사후 확률` : 특정 사건 발생 후 다른 사건이 발생할 확률로 사건 A를 사전에 일어날 사건, 사건 B를 사건 A 발생 이후에 일어날 사건이라고 가정한다면 사건 B의 확률은 다음과 같이 표현된다.

![image](https://user-images.githubusercontent.com/57162812/149853401-62ff6dc9-9afa-4987-9bf3-5ba3e54025a0.png)

`베이즈 정리(Bayes' Theorem)` : 사전 확률과 사후 확률 사이의 관계를 정리한 식이다. 베이즈 정리는 표본공간 S가 다음과 같이 서로 배타적인 사건 <img src="https://latex.codecogs.com/svg.latex?\small&space;A_i" />들로 구성되어 있다고 가정한다.

<p align="center"><img src="https://user-images.githubusercontent.com/57162812/149853646-961a90af-3a3a-40db-8852-14bcb4b05606.png">

사건B 발생에 따른 사건 <img src="https://latex.codecogs.com/svg.latex?\small&space;A_i" />의 조건부 확률은 다음과 같다. 

<p align="center"><img src="https://user-images.githubusercontent.com/57162812/149854229-b980bbe4-9050-48a4-9171-afae70cea770.png">

참고문헌
- [조건부확률, 사후확률, 베이즈정리-확률과 통계(2) - EG공간](https://kongdols-room.tistory.com/133)

---
### No.6
**공분산과 상관계수는 무엇일까요? 수식과 함께 표현해주세요.**

`공분산(Covariance)` : 두 개 이상의 랜덤 변수에 대한 의존성으로 <img src="https://latex.codecogs.com/svg.latex?\small&space;Cov(X,Y)=E((X-E(X)(Y-E(Y))))" />의 공식을 가지고 있다.

 - <img src="https://latex.codecogs.com/svg.latex?\small&space;Cov(X,Y)>0" /> : X가 증가할 때, Y도 증가한다.
 - <img src="https://latex.codecogs.com/svg.latex?\small&space;Cov(X,Y)=0" /> : 두 변수간에는 아무런 선형관계가 없다. 그렇다고 해서 독립적이라고 결단할 수는 없다. (X, Y가 독립이라면 공분산은 0이다.)
 - <img src="https://latex.codecogs.com/svg.latex?\small&space;Cov(X,Y)<0" /> : X가 증가할 때, Y는 감소한다.

공분산에는 하나의 문제점이 있는데 X와 Y의 단위의 크기에 영향을 받는다는 것이다. 이를 보완하기 위한 것이 상관계수이다.

`상관계수(Correlation)` : 확률변수의 절대적 크기에 영향을 받지 않도록 단위화 시켰다. 즉, 분산의 크기만큼 나눈다.

![image](https://user-images.githubusercontent.com/57162812/149855448-ec87ae28-1efe-454a-bd46-574c8cfa8787.png)

 - 상관계수는 -1과 1 사이의 값이다.
 - 확률변수 X, Y가 독립이라면 상관계수는 0이다.
 - X와 Y가 양의 선형관계라면 1, 음의 선형관계라면 -1이다.

참고 문헌
- [Covariance (COV: 공분산)란? - 블로그 - 네이버](https://blog.naver.com/sw4r/221025662499)
- [공분산(Covariance)과 상관계수(Correlation)](https://destrudo.tistory.com/15)
  
---

### No.7
**신뢰 구간의 정의는 무엇인가요?**

`신뢰 구간(Confidence Interval)` : 모수가 어느 범위 안에 있는지를 확률적으로 보여주는 방법으로 샘플링된 데이터를 기반으로 모수 범위를 추정하기 위해 사용된다. 따라서, 신뢰 구간은 샘플링된 표본이 연구중인 모집단을 얼마나 잘 대표하는지 측정하는 방법이라 할 수 있다. 신뢰구간에 모집단 실제 평균값이 포함될 확률을 **신뢰수준(Confidence Level)** 이라 한다.

참고문헌
- [신뢰구간 - 위키백과](https://ko.wikipedia.org/wiki/%EC%8B%A0%EB%A2%B0_%EA%B5%AC%EA%B0%84)
- [신뢰구간 (Confidence Interval)](https://bioinformaticsandme.tistory.com/256)
  
---

### No.8
**p-value를 모르는 사람에게 설명한다면 어떻게 설명하실 건가요?**

`귀무 가설` : 처음 세운 가설  
`대립 가설` : 귀무 가설과 반대되는 실험자가 입증해야하는 가설  
`1종 오류` : 귀무가설이 참인데 기각(대립 가설을 채택)한 경우  

p-value란 1종 오류를 범할 확률이다.

예를 들어, p-value가 0.05(5%)라는 말은 100번 중 95번은 1종 오류를 범하지 않았고, 5번만 1종 오류는 범했다는 말이다. 즉, 95%의 신뢰도(신뢰수준)로 귀무가설을 기각한다는 말이된다.

검정시 1종 오류 확률의 최대 허용치, 즉 유의수준(α)를 정하게 되는데, p-value가 유의 수준보다 작으면 귀무 가설을 기각하고 "유의 수준 α 하에서 유의미하다."고 말한다. 또한, 반대이면 귀무가설을 기각하지 않고 "유의 수준 α하에서 유의미하지 않다"고 말한다.

참고문헌 
- [통계, 기본 개념을 정리해보자 - 이지훈](https://brunch.co.kr/@jihoonleeh9l6/34)
- [[기초통계학] 2편. 가설검정(+p-value, 유의수준 등)에 대한 직관 ](https://soohee410.github.io/stat2)

---

### No.9
**R square의 의미는 무엇인가요?**

<img src="https://latex.codecogs.com/svg.image?R^2" title="R^2" />(결정계수)는 선형 회귀 모델의 분산 설명력이라고 볼 수 있다. 다시 말해, 우리가 만든 모델(회귀선)이 얼마나 데이터를 잘 설명하는지를 의미하는 것이다. 결정계수는 0에서 1 사이의 값을 가질 수 있고, 만약 결정계수가 1이라면 회귀선으로 모든 데이터를 설명할 수 있다고 이해할 수 있다.

결정계수 외에도 회귀 모델의 성능을 평가하는 방법은 MAE, MSE, RMSE 등이 있다.


- <img src="https://latex.codecogs.com/svg.image?R^2" title="R^2" /> 계산법  

  <img src="https://latex.codecogs.com/svg.image?R^2&space;=&space;1&space;-\frac{SSE}{SST}" /> 

    이때, 
    - SSE(Error Sum of Squares) = <img src="https://latex.codecogs.com/svg.image?\sum(y_i-\hat{y}_i)^2" title="\sum(y_i-\hat{y}_i)^2" /> (관측값에서 추정값을 뺀 값. 즉 잔차(residual)의 총합) 
    - SST(Total Sum of Squares) = <img src="https://latex.codecogs.com/svg.image?\sum(y_i-\bar{y}_i)^2" title="\sum(y_i-\bar{y}_i)^2" />  (관측값에서 관측값의 평균을 뺀 결과의 총합)



- <img src="https://latex.codecogs.com/svg.image?R^2" title="R^2" />는 0부터 1까지만 존재
  - <img src="https://latex.codecogs.com/svg.image?R^2=0" title="R^2=1" /> : 모델 설명력이 0
  - <img src="https://latex.codecogs.com/svg.image?R^2=1" title="R^2=0" /> : 모델 설명력이 100%

참고문헌 
- [R2를 어떻게 해석해야 할까-회귀분석 - Sapientia a Dei](https://www.youtube.com/watch?v=__SRJAPvR_k)
- [회귀계수의 의미 , 희귀계수 검정](https://acdongpgm.tistory.com/70)


---

### No.10
**평균(mean)과 중앙값(median)중에 어떤 케이스에서 뭐를 써야할까요?**

- 평균(mean): 모든 데이터의 합을 데이터의 수로 나눈 값
- 중앙값(median): 모든 데이터를 오름차순 혹은 내림차순으로 정렬했을 때 중앙에 해당되는 값

데이터의 분포에 따라 평균과 중앙값 중 어떤 것을 써야 할지 결정될 수 있다. 모든 데이터를 고려한 값인 평균이 일반적으로 중심위치의 척도로 가장 먼저 선택된다. 하지만 아주 큰 관측값이나 아주 작은 관측값(outlier)에 영향을 많이 받는다.

반면 중앙값은 크기가 아닌 순서에 의해 결정되기 때문에 극단적인 관측값에 영향을 받지 않는다. 이처럼 표본의 편차, 혹은 왜곡이 심하게 나타나는 경우 중앙값이 평균보다 유용하다.

참고문헌 
- [[기초통계]평균 중앙값 최빈값 비교(Mean VS Median VS Mode) - 슈퍼짱짱](https://leedakyeong.tistory.com/entry/%ED%8F%89%EA%B7%A0-%EC%A4%91%EC%95%99%EA%B0%92-%EC%B5%9C%EB%B9%88%EA%B0%92-%EB%B9%84%EA%B5%90-Mean-VS-Median-VS-Mode)
- [평균과 중앙값의 차이](https://ko.gadget-info.com/difference-between-mean)

---

### No.11
**중심극한정리는 왜 유용한걸까요?**

중심극한정리란 모집단에서 표본의 크기가 n(일반적으로 30 이상)인 표본추출이 무수히 많이 되는 경우, 표본평균 <img src="https://latex.codecogs.com/svg.image?\overline{X}" title="\overline{X}" />의 분포가 정규분포에 가까워진다는 것이다. 

이러한 정리가 유용한 이유는, 모집단의 분포에 상관 없이 표본평균의 분포가 근사적으로 정규분포를 따르며, 이 사실을 이용하여 모평균에 대한 통계적 추정과 검정이 가능해지기 때문이다. 즉, 모집단 전체를 조사하지 않아도 표본을 여러개 뽑을 수 있다면, 그 표본들의 평균을 이용해서 모집단의 평균과 분산을 추정할 수 있는 것이다. 

참고문헌 
- [중심극한정리의 의미 - 공돌이의 수학정리노트](https://angeloyeo.github.io/2020/09/15/CLT_meaning.html)
- [중심극한정리](https://m.blog.naver.com/exactmehta/80143457287)


---

### No.12
**엔트로피(entropy)에 대해 설명해주세요. 가능하면 Information Gain도요.**

엔트로피는 불확실성, 혹은 데이터의 혼잡도를 의미한다.
확률적으로 불확실성이 크면 엔트로피가 크고, 확률적으로 불확실성이 작으면 엔트로피도 작아진다. 즉 확률이 한쪽으로 치우쳐져 있으면 엔트로피가 작아지고, 확률이 분산되어 있으면 어떤 데이터가 나올지 모르므로 엔트로피가 커지게 된다.

엔트로피는 다음과 같이 데이터가 어떤 클래스에 속할 확률에 대한 기댓값으로 표현할 수 있다. 

<img src="https://latex.codecogs.com/svg.image?E&space;=&space;-\sum_{i=1}^{k}p_ilog_2(p_i)" title="E = -\sum_{i=1}^{k}p_ilog_2(p_i)" />

</br>

다음은 엔트로피 계산 예시이다.

* 10개의 동전 중 500원짜리 5개, 100원짜리 5개가 있을 경우  
  <img src="https://latex.codecogs.com/svg.image?E(5,5)=-\frac{5}{10}log_2\frac{5}{10}-\frac{5}{10}log_2\frac{5}{10}=1" title="E(5,5)=-\frac{5}{10}log_2\frac{5}{10}-\frac{5}{10}log_2\frac{5}{10}=1" />  

* 10개 동전 중 500원짜리가 10개 있을 경우  
  <img src="https://latex.codecogs.com/svg.image?E(10)=-\frac{10}{10}log_2\frac{10}{10}=0" title="E(10)=-\frac{10}{10}log_2\frac{10}{10}=0" />

Information Gain이란 특정 속성을 기준으로 데이터를 구분하게 될 때, 감소되는 entropy의 양을 의미한다. 

즉, 특정 속성 A의 Information Gain = 전체 데이터의 엔트로피 - 특정 속성 A로 분류 시 엔트로피가 된다.

참고문헌
- [[인공지능] 엔트로피(Entropy) 와 정보이득(Information Gain) 계산 - 꾸준희](https://eehoeskrap.tistory.com/13)
- [정보이득 information Gain](https://www.youtube.com/watch?v=_ExYGz3HbJI)
---

### No.13
**어떨 때 모수적 방법론을 쓸 수 있고, 어떨 때 비모수적 방법론을 쓸 수 있나요?**

통계적 추론: 표본의 통계량(평균, 표준편차 등)을 통해 모집단의 모수(모평균, 모표준편차 등)을 추정하는 방법

궁극적으로 알고 싶은 전체 집단을 모집단이라고 한다. 하지만 현실적으로 전체 집단을 모두 조사하는 것은 불가능하므로, 적절한 표본 집단을 지정하여 이 표본집단에서 평균, 표준편차와 같은 통계량을 구한 뒤 이를 통해 모평균과 모표준편차를 추정해야 한다.

모수적 방법은 모집단이 어떤 분포를 따른다는 가정 하에 통계적 추론을 하는 방법이다. 일반적으로 표본의 수가 30개 이상일 때 중심극한정리에 의해 표본평균이 정규분포를 따르게 되므로 모수적 방법론을 사용할 수 있다. 

반면, 관측값이 어느 특정한 확률분포를 따른다고 전제할 수 없거나(표본의 수가 매우 적을 경우 등) 모집단에 대한 아무런 정보가 없는 경우 비모수적 방법론을 사용한다. 비모수적 방법론은 순위(rank)나 부호(sign)에 기초한 방법인데, 머신러닝 관점에서 K-Nearest Neighbors 알고리즘이 대표적인 비모수적 모델이다.


참고문헌
- [[통계이론] 모수적 방법 vs 비모수적 방법](https://zzanhtt.tistory.com/18)
- [모수 모델 vs. 비모수 모델](https://brunch.co.kr/@seoungbumkim/7)

---

### No.14
**“likelihood”와 “probability”의 차이는 무엇일까요?**

- 확률(Probability): 고정된 분포 내에서 특정 데이터가 출현할 가능성이다.(분포곡선 아래의 넓이)  
- 가능도(Likelihood): 변할 수 있는 분포에서 고정된 데이터에 해당하는 y축 값이다.

아래 예시를 보면 쉽게 이해할 수 있다.
쥐의 몸무게는 평균 32(gram), 표준편차 2.5인 정규분포를 따른다고 하자. 만약 임의로 선택한 쥐의 몸무게가 32\~34일 확률은, 분포곡선에서 32\~34 사이의 넓이가 될 것이다.

즉 확률은 아래의 식으로 계산된다.  
  * <img src="https://latex.codecogs.com/svg.image?pr(&space;32&space;\sim&space;34&space;&space;|&space;N(32,&space;2.5^2))" title="pr( 32 \sim 34 | N(32, 2.5^2))" />

다른 몸무게에 대해 확인해보고 싶다면 식에서 오른쪽의 조건을 변경해주면 된다. 하지만 왼쪽의 분포 조건은 바뀌지 않는다.

![pro](https://user-images.githubusercontent.com/63924704/150070433-d56c107a-bcce-451e-813a-f20b86d5ea9b.jpeg)

</br>

반면 가능도에서는 이미 특정 쥐에 대한 몸무게를 측정했다고 가정한다. 몸무게가 34그램인 쥐가 관측되었다면, <img src="https://latex.codecogs.com/svg.image?N(32,&space;2.5^2)" title="N(32, 2.5^2)" /> 에서 이 데이터에 해당하는 값은 0.12이고 이것이 가능도가 된다.

이때 정규분포를 오른쪽으로 이동하여 <img src="https://latex.codecogs.com/svg.image?N(34,&space;2.5^2)" title="N(34, 2.5^2)" />를 따르게 한다면, 이에 해당하는 가능도는 0.21이 된다.

![lik](https://user-images.githubusercontent.com/63924704/150070522-e57e767c-b2fc-40eb-ad5c-d29970ac2f72.jpeg)

</br>

마지막으로 식을 정리하면 아래와 같다. 

- 확률=  <img src="https://latex.codecogs.com/svg.image?pr(&space;\textrm{data}(\textbf{changable})&space;|&space;\textrm{distribution}(\textbf{fixed})&space;)&space;" title="pr( \textrm{data}(\textbf{changable}) | \textrm{distribution}(\textbf{fixed}) ) " />   

- 가능도= <img src="https://latex.codecogs.com/svg.image?L(&space;\textrm{distribution}(\textbf{changable})&space;|&space;\textrm{data}(\textbf{fixed})&space;)&space;" title="L( \textrm{distribution}(\textbf{changable}) | \textrm{data}(\textbf{fixed}) ) " />



참고 문헌
- [Probability is not Likelihood - StatQuest with Josh Starmer](https://www.youtube.com/watch?v=pYxNSUDSFH4)

---

### No.15
**통계에서 사용되는 bootstrap의 의미는 무엇인가요.**

궁극적으로 알고 싶은 모집단이 있고, 이 모집단의 표본을 추출했다고 하자. 현실에서는 비용 등의 여러가지 문제로 모집단의 또 다른 표본을 추출할 수 없는 경우가 많다. 이때 표본의 표본을 무수히 많이 복원추출하여 표본에 대한 통계적 추정을 하고, 모집단에 대한 추정까지 가능하게 하는 것이 bootstrap이다. 샘플에서 샘플을 뽑는다 하여 re-sampling이라고도 부른다. 

중심극한정리는 모집단에서 표본을 여러개 뽑는 것이고, bootstrap은 현재 우리가 갖고 있는 표본에서 표본을 추출하는 것이다. 또한, 중심극한정리에서는 일반적으로 표본추출이 30번 이상 이루어지면 가능하지만, bootstrap은 최소 1000개 이상의 표본을 추출해야한다.  

참고 문헌
- [중심극한 정리와 Bootstrap](https://www.youtube.com/watch?v=g8Tr_Vn7xtE)

---

### No.16
**데이터가 매우 적은 (수십개 이하) 케이스의 경우 어떤 방식으로 예측 모델을 수립할 수 있을까요?**

표본이 매우 작은 경우 표본평균의 분포가 정규분포를 따른다고 가정할 수 없으므로 비모수적 방법을 채택하여 예측 모델을 수립할 수 있다. 하지만 중심극한정리에 의해 표본의 크기가 30보다 클 경우 표본평균이 정규분포를 따른다고 가정할 수 있으므로, 이 경우에는 모수적 방법을 사용한다.

참고 문헌
- [[통계이론] 모수적 방법 vs 비모수적 방법](https://zzanhtt.tistory.com/18)

---

### No.17

**베이지안과 프리퀀티스트 간의 입장차이를 설명해주실 수 있나요?**

프리퀀티스트는 어떤 사건에 대한 확률 또는 어떤 모집단의 기대값 등의 특성이 고정되어 있다고 믿으며, 반복적인 실험의 시행 또는 충분히 큰 표본을 통해서 해당 값에 근사할 수 있다고 믿는다. 고정되어 있는 확률 또는 기대값에 충분히 근접하는지(rf. 신뢰구간)에 따라 해당 '가설'을 채택하거나 기각한다.

베이지안은 확률을 고정값으로 보지 않는다. 주관적인 사전 확률을 사용하며, 실험 후의 사후 확률로 사전 확률을 고쳐나간다.

예를 들어, 어떤 주사위가 있다고 하자. 이 주사위에 붙어있던 스티커가 하나 떨어져서 5가 없고 4가 두 면에 나타나있다. 프리퀀티스트와 베이지안은 이 주사위를 굴려서 4가 나올 확률이 1/6이라고 가정할 것이다. 프리퀀티스트는 이 주사위를 여러번 굴려 확률을 계산하고, 실험 결과 확률값이 1/6에 근접하지 않는다는 것을 확인하고 자신의 가설을 기각할 것이다. 반면, 베이지안은 주사위를 한번씩 굴릴 때마다 자신이 생각하던 사전 확률 (1/6)을 수정해나갈 것이다.

참고 문헌

- [베이지안 패러다임](https://freshrimpsushi.github.io/posts/bayesian-paradigm/)
- [Frequentist and Bayesian](https:/www.ibric.org/myboard/read.php?Board=sori&id=19818)
- [IS CHESS A GAME OF CHANCE? Classical vs Frequentist vs Bayesian Probability](https://www.youtube.com/watch?v=dejy4PPCFHY)
- [Frequentist and Bayesian Approaches in Statistics](https://www.probabilisticworld.com/frequentist-bayesian-approaches-inferential-statistics/)
- [기초통계학[34].베이즈 추론과 빈도주의 추론의 비교](https://everyday-image-processing.tistory.com/78)

---

### No.18

**검정력(statistical power)은 무엇일까요?**

![type_1_2_error](https://www.nutritiontactics.com/wp-content/uploads/2018/11/01.jpg)

1종 오류(알파 오류)는 귀무가설(영가설)이 실제로 참이지만, 이에 불구하고 귀무가설을 기각하는 오류이다. 즉, 실제 음성인 것을 양성으로 판정하는 경우이다. 2종 오류(베타 오류)는 귀무가설이 실제로 거짓(대립가설이 실제로 참)이지만, 이에 불구하고 귀무가설을 채택하는 오류이다. 즉, 실제 양성인 것을 음성으로 판정하는 경우이다.
p-value는 1종 오류와 유관하며 검정력은 2종 오류와 유관하다. 검정력은 잘못된 귀무가설을 (올바르게) 기각할 확률을 뜻한다.

참고 문헌

- [(위키) 1종 오류와 2종 오류](https://ko.wikipedia.org/wiki/1%EC%A2%85_%EC%98%A4%EB%A5%98%EC%99%80_2%EC%A2%85_%EC%98%A4%EB%A5%98)
- [(위키) 검정력](https://ko.wikipedia.org/wiki/%EA%B2%80%EC%A0%95%EB%A0%A5)
- [No significant difference? Oh really? How to recognize weaknesses in muscle hypertrophy studies.](https://www.nutritiontactics.com/strength-training-studies-underpowered-ironic-muscle-hypertrophy-stats-explained/)
- [A Gentle Introduction to Statistical Power and Power Analysis in Python](https://machinelearningmastery.com/statistical-power-and-power-analysis-in-python/)

---

### No.19

**missing value가 있을 경우 채워야 할까요? 그 이유는 무엇인가요?**

XGBoost와 같은 특수 케이스의 머신러닝 모델이 아니라면 missing value를 그대로 두는 것은 허용되지 않으며 에러를 유발한다. 결측치를 채우지 않고 해당 피쳐 column을 지우거나 해당 data point row를 지우는 방법도 존재하나, 적절한 값으로 채우는 것이 가장 선호되는 편이며, missing value를 채워 넣었을 때 많은 경우에 성능 향상으로 이어지기도 한다.

결측치 대체(imputation) 방법으로는 다양한 방법들이 존재한다.
1. 피쳐 column의 평균값 또는 중앙값으로 대체할 수 있다. 적은 연산으로 작업할 수 있어 쉽고 빠르며, 연속변수 데이터에 잘 맞는 편이다. 하지만 categorical 변수들에는 사용할 수 없는 단점이 있고, 성능 또한 뛰어나지는 못한 편이다.
2. 최빈값 또는 0과 같은 특정 상수값으로 대체할 수도 있다. categorical 변수들에도 적용 가능하단 장점이 있지만 데이터에 편향을 가할 수 있다는 단점이 있다.
3. linear regression, KNN과 같은 머신러닝 모델 혹은 조금 더 복잡한 딥러닝 모델을 사용하여 결측치 값을 예측하여 채워넣는 방법 또한 존재한다. 성능이 가장 우수한 편에 속하지만, 비용이 많이 발생하는 단점이 존재한다. 

참고 문헌

- [6 Different Ways to Compensate for Missing Values In a Dataset (Data Imputation with examples)](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779)
- [Dealing With Missing Values in Python – A Complete Guide](https://www.analyticsvidhya.com/blog/2021/05/dealing-with-missing-values-in-python-a-complete-guide/)

---

### No.20

**아웃라이어의 판단하는 기준은 무엇인가요?**

아웃라이어는 다른 data point들로부터 멀리 떨어져있는 data point며 데이터 전반적인 분포에서 많이 벗어나 있는 값을 말한다.

이상치를 가장 찾기 쉽고 단순한 방법 중 하나로는 히스토그램이나 scatterplot 등의 시각화자료를 통해 눈으로 직접 확인하는 것이다. 이 방법을 통해 쉽게 이상치 유무 및 이상치의 양 등을 확인할 수 있다.
두번째로는 InterQuartile range (IQR) 및 boxplot을 사용한 방법이 있다. 사분위수들 중 제1사분위수(Q1)과 제3사분위수(Q3) 사이 값을 IQR이라고 부르며, 데이터셋의 중앙 50%에 해당하는 값들이 이 구간에 속해 있다. 그리고 Q1과 Q3 기반으로 계산되는 상한선 및 하한선을 넘어서는 값들을 이상치로 본다.
또 다른 방법으로는 표준편차 및 Z-score를 이용하는 방식이 있다. 데이터가 정규분포를 띄고 있다면 99.7%의 데이터가 평균±3*표준편차 범위 이내에 분포하고 있다. Z-score는 평균으로부터 몇 표준편차만큼 해당 데이터가 떨어져있는지를 알려주는 측도로 사용된다.
다양한 통계적 검정 과정을 거치는 방법도 존재한다. 그럽스 검정(Grubbs' Test)과 딕슨 Q검정(Dixon’s Q test)이 정규분포 내 이상치를 찾는데 사용되며, 데이터가 카이분포를 띄고 있다면 카이검정(Chi-square test)도 사용 가능하다.
이 밖에 DBScan 군집화 등 머신러닝 모델을 이용한 이상치 탐지도 가능하다.

참고 문헌

- [5 Ways to Find Outliers in Your Data](https://statisticsbyjim.com/basics/outliers/)
- [5 Ways to Detect Outliers/Anomalies That Every Data Scientist Should Know (Python Code)](https://towardsdatascience.com/5-ways-to-detect-outliers-that-every-data-scientist-should-know-python-code-70a54335a623)
- [Outliers in data and ways to detect them.](https://medium.com/analytics-vidhya/outliers-in-data-and-ways-to-detect-them-1c3a5f2c6b1e)

---

### No.21

**필요한 표본의 크기를 어떻게 계산합니까?**

필요한 표본의 크기의 계산식은 모집단에 대해 알고 있는 정보에 따라서 달라진다. 

Z가 신뢰수준(신뢰구간에 대한 z-값), e가 오차범위, p가 조사특성값을 가질 비율의 추정치라고 할 때, 모집단의 population이 충분히 크다면,
<p align="center"><img src="https://latex.codecogs.com/svg.image?n_0&space;=&space;\frac{Z^2p(1-p)}{e^2}" title="n_0 = \frac{Z^2p(1-p)}{e^2}" /></p>
로 필요한 표본의 크기를 계산할 수 있다. (Chochran 공식)

만일 연구 대상 모집단의 population이 작다면 모집단 population의 크기 N을 이용해서 위 공식을 수정하여,
<p align="center"><img src="https://latex.codecogs.com/svg.image?n=\frac{n_0}{1&plus;\frac{(n_0-1)}{N}}=\frac{N*n_0}{n_0&plus;N-1}" title="n=\frac{n_0}{1+\frac{(n_0-1)}{N}}=\frac{N*n_0}{n_0+N-1}" /></p>
로 필요한 표본의 크기를 계산할 수 있다. (수정된 Chochran 공식)

예를 들어, 신뢰구간 95% 내에 남녀비율이 50:50인 모집단을 올바르게 대표할 수 있는 표본의 크기를 계산한다고 하자.
<p align="center"><img src="https://latex.codecogs.com/svg.image?n_0&space;=&space;\frac{{Z_{\frac{0.95}{2}}}^2p(1-p)}{e^2}&space;=&space;\frac{1.96^2\times0.5\times(1-0.5)}{(1-0.95)^2}=384.15" title="n_0 = \frac{{Z_{\frac{0.95}{2}}}^2p(1-p)}{e^2} = \frac{1.96^2\times0.5\times(1-0.5)}{(1-0.95)^2}=384.15" /></p>
로 계산되어 385명 이상의 표본이 필요하다는 것을 알 수 있게 된다. 이를 두번째 공식으로 조정하게 된다면(인구 수가 10만명이라고 가정),
<p align="center"><img src="https://latex.codecogs.com/svg.image?n=\frac{n_0}{1&plus;\frac{(n_0-1)}{N}}=\frac{384.15}{1&plus;\frac{(384.15-1)}{100000}}=382.68" title="n=\frac{n_0}{1+\frac{(n_0-1)}{N}}=\frac{384.15}{1+\frac{(384.15-1)}{100000}}=382.68" /></p>
로 수정된다.
전체 모집단의 인구수가 커질 수록 보정되는 필요 표본의 양은 더 작아지며, 반대로 모집단의 인구수가 작아질 수록 두번째 공식에 의해 더 많이 보정된다.

> ※ 많은 국내 웹문서에서 인용되고 있는 [surveymonkey의 계산기 공식](https://www.surveymonkey.com/mp/sample-size-calculator/)은 분모에서 -1이 빠진 형태를 띄고 있으며 이에 대한 설명을 찾아볼 수 없었음.

참고로 이는 전통적인 통계적 방식을 사용할 때의 방법이며, 빅데이터 및 인공지능 연구에서 Chochran 공식의 사용은 찾아볼 수 없었다.

참고 문헌

- [How to Sample Data (With Code)](https://towardsdatascience.com/how-to-sample-data-with-code-327359dce10b)
- [Sample Size in Statistics (How to Find it): Excel, Cochran’s Formula, General Tips](https://www.statisticshowto.com/probability-and-statistics/find-sample-size/)
- [Population Proportion – Sample Size](https://select-statistics.co.uk/calculators/sample-size-calculator-population-proportion/)
- [[통계] 모집단크기/오차범위/신뢰수준으로 표본수 계산하기](https://m.blog.naver.com/haam91/220668652939)
- [17. 표본크기의 결정](https://swmh.tistory.com/124)
- [How Much Training Data is Required for Machine Learning?](https://machinelearningmastery.com/much-training-data-required-machine-learning/)

---

### No.22

**Bias를 통제하는 방법은 무엇입니까?**

bias가 높다는 것은 모델의 예측값이 전반적으로 실제값과 멀게 예측되었다는 것을 뜻하며, bias가 낮다는 것은 모델의 예측값이 전반적으로 실제값과 가깝게 예측되었다는 것을 뜻한다. 보통 variance와 tradeoff 관계에 있다고 한다. 높은 bias는 underfitting과 관계가 있다.

<img src="https://gaussian37.github.io/assets/img/ml/concept/bias_and_variance/17.png" alt="tradeoff" style="zoom:70%;" />
<img src="https://gaussian37.github.io/assets/img/ml/concept/bias_and_variance/18.png" alt="tradeoff2" style="zoom:80%;" />

bias를 통제하기 위한 방법으로는 다음과 같은 방법들이 있다.

1. 모델의 크기를 증가시킨다: 모델의 파라미터를 증가시키면 더 복잡한 데이터 간 관계를 학습 시킬 수 있다.
2. 모델에 가해지는 규제(regularization)을 약화시킨다: regularization이 가해질 수록 모델은 편향된 데이터에 덜 민감하게 반응을 하게 되고 variance가 낮아진다. (심한 경우 underfitting을 유발한다.) 반대로 regularization이 약화될 수록 편향된 데이터에 더 민감하게 반응하여 bias가 낮아진다. (심한 경우 overfitting을 유발한다.)
3. learning rate을 줄여보는 등 하이퍼파라미터를 변경하거나, (비선형) 활성화 함수의 종류 등을 변경해본다.
4. 새로운 피쳐를 추가해본다.

참고 문헌

- [머신러닝에서의 Bias와 Variance](https://gaussian37.github.io/machine-learning-concept-bias_and_variance/)
- [Bias and Variance: Two Important Machine Learning Concepts to Improve Every Model](https://towardsdatascience.com/two-important-machine-learning-concepts-to-improve-every-model-62fd058916b)

---

### No.23

**로그 함수는 어떤 경우 유용합니까? 사례를 들어 설명해주세요.**

로그함수는 영화평 댓글 수와 같이 한쪽으로 치우쳐진 피쳐의 skewness(왜도)를 줄일 때 유용하다. 

![ratings_per_movie](https://developers.google.com/machine-learning/data-prep/images/norm-log-scaling-movie-ratings.svg)

이 과정을 통해 정규성을 높이고 선형모델들의 성능을 높일 수 있다.

참고 문헌

- [Log Scaling | Normalization](https://developers.google.com/machine-learning/data-prep/transform/normalization#log-scaling)
- [Logarithms — What, Why and How](https://towardsdatascience.com/logarithms-what-why-and-how-ff9d050d3fd7)
- [데이터 분석 시 식에 로그를 취하는 이유](https://leebaro.tistory.com/entry/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%84%EC%84%9D-%EC%8B%9C-%EC%8B%9D%EC%97%90-%EB%A1%9C%EA%B7%B8%EB%A5%BC-%EC%B7%A8%ED%95%98%EB%8A%94-%EC%9D%B4%EC%9C%A0)

---

### No.24

**베르누이 분포 / 이항 분포 / 카테고리 분포 / 다항 분포 / 가우시안 정규 분포 / t 분포 / 카이제곱 분포 / F 분포 / 베타 분포 / 감마 분포에 대해 설명해주세요. 그리고 분포 간의 연관성도 설명해주세요.**

이산형 확률 분포로는 베르누이 분포, 이항분포, 카테고리 분포, 다항 분포 등이 있으며, 연속형 확률 분포로는 가우시안 정규 분포, (스튜던트) t 분포, 카이제곱 분포, F 분포, 감마분포, 베타 분포 등이 있다.

**이산 확률 변수**로는 결과가 binary인 **베르누이 확률변수**를 대표적인 예시로 들 수 있으며, 베르누이 확률변수는 **베르누이 시행**의 결과를 0 또는 1로 바꾼 것이다. 동전 뒤집기를 베르누이 시행의 예시로 들 수 있다.



**베르누이 분포**는 동전을 던졌을 때의 결과의 분포로 생각할 수 있다.

![bernoulli](https://github.com/rasmusab/distribution_diagrams/raw/master/png/Bernoulli.png)

**이항분포**는 베르누이 시행을 N번 (복원) 반복하였을 때 성공이 나타난 횟수를 확률변수로 하는 확률 분포이다.

![binomial](https://github.com/rasmusab/distribution_diagrams/raw/master/png/binomial.png)



**카테고리분포**와 **다항분포**는 각각 베르누이분포와 이항분포의 확장판이라고 볼 수 있다. **카테고리 확률 변수**는 결과값으로 1부터 K까지 K개 정수값 중 하나를 가진다고 할 때, 0 또는 1로만 이루어진 다차원 벡터를 출력하는 것이 특징이기도 하다. (예/ x =1 -> x = (1,0,0,0,0,0))

![categorical](https://github.com/rasmusab/distribution_diagrams/raw/master/png/categorical.png)

**다항분포**는 결과값으로 1부터 K까지 K개 정수값 중 하나를 가지는 실험을 N번 반복 시행했을 때의 분포다.



**연속 확률 변수**의 확률 분포 중 대표적인 예시는 **정규분포**이며 자연 현상에서 나타나는 숫자를 확률 모형으로 모형화할 때 많이 사용한다. 정규분포는 평균과 표준편차로 그 모양이 정의되며, 평균이 0, 표준편차가 1인 정규분포는 **표준정규분포**라고 한다.

![normal](https://github.com/rasmusab/distribution_diagrams/raw/master/png/normal.png)



**t 분포**는 정규분포와 유사하지만 양 끝단의 꼬리가 더 두꺼운 분포이다. 현실의 데이터가 이 t분포를 띄고 있는 경우가 많다. (표본의 수와 관련된) 자유도가 증가할수록 표준정규분포에 가까워지며, 보통 표본수가 30 이상일 때 표준정규분포와 가깝게 처리한다고 한다. 반대로 표본의 크기가 30보다 작으면 t분포를 이용해야 한다.

![t](https://github.com/rasmusab/distribution_diagrams/raw/master/png/t_distrib.png)

t분포가 정규분포를 따르는 확률 변수의 표본들의 합을 표본 분산으로 정규화한 결과라면, **카이제곱분포**는 표본들을 단순히 더하는 것이 아니라 제곱을 하여 더하여 양수값만을 가지도록 유도하여 얻은 분포이다. 자유도 모수에 따라 모양이 크게 변한다.

![chisquare](https://github.com/rasmusab/distribution_diagrams/raw/master/png/chisquare.png)

**F 분포**는 두 가지 이상의 표본집단의 분산을 비교하거나 모집단의 분산을 추정할 때 쓰이는 분포이다. 기본적인 모양은 카이제곱분포의 모양을 띄고 있다. 비교 대상이 되는 선형모형의 오차 제곱합에 대한 비율의 확률 분포로써 사용된다.

![F](https://github.com/rasmusab/distribution_diagrams/raw/master/png/F_dist.png)



**감마분포**와 **베타분포** 모두 모수의 베이지안 추정에 사용된다. **감마분포**는 0부터 무한대의 값을 가지는 양수 값을 추정하는 데 사용되며, **베타분포**는 0부터 1 사이의 값을 가지는 모수를 추정하는데 사용된다. 베르누이분포의 모수값 추정과 많은 관련이 있다.

**감마분포**는 팩토리얼을 복소수 영역까지 확장하여 일반화한 결과인 감마함수를 기반으로 하는 분포이다. N번째 사건이 일어날 때 까지 걸리는 시간에 대한 연속 확률분포이다.

![gamma](https://github.com/rasmusab/distribution_diagrams/raw/master/png/gamma.png)

**베타분포**는 0과 1사이의 표본공간에서 정의된다. a와 b 두 모수를 가지며 두 모수에 따라 모양이 결정되기에 두 모수를 형상 인자(shape factor)라고 부른다.

![beta](https://github.com/rasmusab/distribution_diagrams/raw/master/png/beta.png)

이 분포들은 다음과 같은 관계를 갖는다.

![중심극한정리](https://mblogthumb-phinf.pstatic.net/MjAxNjExMDVfMTAy/MDAxNDc4Mjc3Mzk5MjAz.Bi7zkiFvP6xRlOKUXjOA-zxbQ6NtuenTK5KzCVswVeIg.HvJqZTnHVdjOvKUpHQZdZGJ95Fllo1UMUhyPmyIV7uMg.JPEG.mykepzzang/%ED%99%95%EB%A5%A0%EA%B3%BC%ED%86%B5%EA%B3%84.jpg?type=w2)

각 확률분포의 표본 수가 증가할수록 표집분포는 정규분포에 가까운 모양을 띄게 된다.

참고 문헌

- [8.2 베르누이분포와 이항분포](https://datascienceschool.net/02%20mathematics/08.02%20%EB%B2%A0%EB%A5%B4%EB%88%84%EC%9D%B4%EB%B6%84%ED%8F%AC%EC%99%80%20%EC%9D%B4%ED%95%AD%EB%B6%84%ED%8F%AC.html)
- [[확률과 통계] 29. 이산확률분포(1) - 베르누이 분포, Bernoulli Distribution](https://m.blog.naver.com/mykepzzang/220838870172)
- [[확률과 통계] 30. 이산확률분포(2) - 이항 분포, Binomial Distribution](https://m.blog.naver.com/mykepzzang/220838887115)
- [8.3 카테고리분포와 다항분포](https://datascienceschool.net/02%20mathematics/08.03%20%EC%B9%B4%ED%85%8C%EA%B3%A0%EB%A6%AC%EB%B6%84%ED%8F%AC%EC%99%80%20%EB%8B%A4%ED%95%AD%EB%B6%84%ED%8F%AC.html)
- [[확률과 통계] 31. 이산확률분포(3) - 다항 분포, Multinomial Distribution](https://m.blog.naver.com/mykepzzang/220838897399)
- [8.4 정규분포와 중심극한정리](https://datascienceschool.net/02%20mathematics/08.04%20%EC%A0%95%EA%B7%9C%EB%B6%84%ED%8F%AC%EC%99%80%20%EC%A4%91%EC%8B%AC%EA%B7%B9%ED%95%9C%EC%A0%95%EB%A6%AC.html)
- [[확률과 통계] 38. 연속확률분포(2) - 정규 분포, Normal Distribution](https://m.blog.naver.com/mykepzzang/220841600720)
- [[확률과 통계] 39. 연속확률분포(3) - 표준 정규 분포, Standard Normal Distribution](https://m.blog.naver.com/mykepzzang/220841610517)
- [8.5 스튜던트 t분포, 카이제곱분포, F분포](https://datascienceschool.net/02%20mathematics/08.05%20%EC%8A%A4%ED%8A%9C%EB%8D%98%ED%8A%B8%20t%EB%B6%84%ED%8F%AC%2C%20%EC%B9%B4%EC%9D%B4%EC%A0%9C%EA%B3%B1%EB%B6%84%ED%8F%AC%2C%20F%EB%B6%84%ED%8F%AC.html)
- [[확률과 통계] 50. t 분포, Student's t-Distribution](https://m.blog.naver.com/mykepzzang/220853827288)
- [[확률과 통계] 49. 카이제곱 분포, Chi-Squared Distribution](https://m.blog.naver.com/mykepzzang/220852102307)
- [[확률과 통계] 51. F 분포, Snedecor's F-Distribution](https://m.blog.naver.com/mykepzzang/220855136935)
- [8.7 베타분포, 감마분포, 디리클레분포](https://datascienceschool.net/02%20mathematics/08.07%20%EB%B2%A0%ED%83%80%EB%B6%84%ED%8F%AC%2C%20%EA%B0%90%EB%A7%88%EB%B6%84%ED%8F%AC%2C%20%EB%94%94%EB%A6%AC%ED%81%B4%EB%A0%88%20%EB%B6%84%ED%8F%AC.html)
- [[확률과 통계] 40. 연속확률분포(4) - 감마 분포, Gamma Distribution](https://m.blog.naver.com/mykepzzang/220842759639)
- [[확률과 통계] 42. 연속확률분포(6) - 베타 분포, Beta Distributuion](https://m.blog.naver.com/mykepzzang/220843077734)
- [[확률과 통계] 48. 중심극한정리, Central Limit Theorem](https://m.blog.naver.com/mykepzzang/220851280035)

---

### No.25

**출장을 위해 비행기를 타려고 합니다. 당신은 우산을 가져가야 하는지 알고 싶어 출장지에 사는 친구 3명에게 무작위로 전화를 하고 비가 오는 경우를 독립적으로 질문해주세요. 각 친구는 2/3로 진실을 말하고 1/3으로 거짓을 말합니다. 3명의 친구가 모두 “그렇습니다. 비가 내리고 있습니다”라고 말했습니다. 실제로 비가 내릴 확률은 얼마입니까?**

A: 실제로 비가 내리는 경우 / B: 세명이 모두 비가 내린다고 말할 경우 라고 할 때, *세명이 모두 비가 내린다고 말을 했을 때 실제 비가 내릴 경우의 조건부확률*을 구해야 한다.

*세명이 모두 비가 내린다고 말했을 경우의 확률*은 (실제로 비가 내리고) *셋 다 진실을 말했을 경우의 확률*과 (실제로 비가 내리고 있지 않고) *셋 다 거짓말을 했을 경우의 확률*의 합과 같다.
<p align="center"><img src="https://latex.codecogs.com/svg.image?P(B)&space;=&space;P(A&space;\cap&space;B)&plus;P(A^c&space;\cap&space;B)&space;=&space;(\frac{2}{3}\times\frac{2}{3}\times\frac{2}{3})&plus;(\frac{1}{3}\times\frac{1}{3}\times\frac{1}{3})" title="P(B) = P(A \cap B)+P(A^c \cap B) = (\frac{2}{3}\times\frac{2}{3}\times\frac{2}{3})+(\frac{1}{3}\times\frac{1}{3}\times\frac{1}{3})" /></p>
*세명이 모두 비가 내린다고 말을 했을 때 실제 비가 내릴 경우의 조건부확률*은 다음과 같다.
<p align="center"><img src="https://latex.codecogs.com/svg.image?P(A|B)&space;=&space;\frac{P(A&space;\cap&space;B)}{P(B)}&space;=&space;\frac{\frac{2}{3}\times\frac{2}{3}\times\frac{2}{3}}{(\frac{2}{3}\times\frac{2}{3}\times\frac{2}{3})&plus;(\frac{1}{3}\times\frac{1}{3}\times\frac{1}{3})}&space;=&space;\frac{8}{9}" title="P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{\frac{2}{3}\times\frac{2}{3}\times\frac{2}{3}}{(\frac{2}{3}\times\frac{2}{3}\times\frac{2}{3})+(\frac{1}{3}\times\frac{1}{3}\times\frac{1}{3})} = \frac{8}{9}" /></p>

